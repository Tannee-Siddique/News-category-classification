{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "375dd455-71a9-459f-9964-2f90430a65a3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "{\n",
    "    \"host\": \"cluster0.qnu8i.mongodb.net\",\n",
    "    \"port\": 27017,\n",
    "    \"username\": \"adothi1997\",\n",
    "    \"password\": \"xm3YvTyKgto0CgVk\"\n",
    "  }\n",
    "df_spark = spark.read.format(\"com.mongodb.spark.sql.DefaultSource\") \\\n",
    "    .option(\"uri\", \"mongodb+srv://adothi1997:xm3YvTyKgto0CgVk@cluster0.qnu8i.mongodb.net/news_dataset.news_collection\") \\\n",
    "    .load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9afc0c2e-357a-4979-8c8b-6acce2d05d58",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------------------+-------------------+------------------+----------------+--------------------+--------------------+--------------+----------+--------------------+---------------+--------------------+--------------------+\n|Author_Article_Count|Average_Headline_Length|     Date_Timestamp|Description_Length|Publication_Year|                 _id|             authors|      category|      date|            headline|headline_length|                link|   short_description|\n+--------------------+-----------------------+-------------------+------------------+----------------+--------------------+--------------------+--------------+----------+--------------------+---------------+--------------------+--------------------+\n|                   1|     58.415354584373375|2022-09-23 00:00:00|               154|            2022|{672046f58761b82b...|Carla K. Johnson, AP|     U.S. NEWS|2022-09-23|Over 4 Million Am...|             76|https://www.huffp...|Health experts sa...|\n|                1566|     58.415354584373375|2022-09-23 00:00:00|               159|            2022|{672046f58761b82b...|      Mary Papenfuss|     U.S. NEWS|2022-09-23|American Airlines...|             89|https://www.huffp...|He was subdued by...|\n|                 650|     58.415354584373375|2022-09-23 00:00:00|                64|            2022|{672046f58761b82b...|       Elyse Wanshel|        COMEDY|2022-09-23|23 Of The Funnies...|             69|https://www.huffp...|\"Until you have a...|\n|                 889|     58.415354584373375|2022-09-23 00:00:00|               159|            2022|{672046f58761b82b...|    Caroline Bologna|     PARENTING|2022-09-23|The Funniest Twee...|             56|https://www.huffp...|\"Accidentally put...|\n|                1052|     58.415354584373375|2022-09-22 00:00:00|               156|            2022|{672046f58761b82b...|      Nina Golgowski|     U.S. NEWS|2022-09-22|Woman Who Called ...|             77|https://www.huffp...|Amy Cooper accuse...|\n|               37418|     58.415354584373375|2022-09-22 00:00:00|               162|            2022|{672046f58761b82b...|                    |     U.S. NEWS|2022-09-22|Cleaner Was Dead ...|             70|https://www.huffp...|The 63-year-old w...|\n|                 650|     58.415354584373375|2022-09-22 00:00:00|               119|            2022|{672046f58761b82b...|       Elyse Wanshel|     U.S. NEWS|2022-09-22|Reporter Gets Ado...|             67|https://www.huffp...|\"Who's that behin...|\n|                   2|     58.415354584373375|2022-09-22 00:00:00|               116|            2022|{672046f58761b82b...|     DÁNICA COTO, AP|    WORLD NEWS|2022-09-22|Puerto Ricans Des...|             65|https://www.huffp...|More than half a ...|\n|                 696|     58.415354584373375|2022-09-22 00:00:00|               148|            2022|{672046f58761b82b...|         Marina Fang|CULTURE & ARTS|2022-09-22|How A New Documen...|             76|https://www.huffp...|In \"Mija,\" direct...|\n|                   5|     58.415354584373375|2022-09-21 00:00:00|               148|            2022|{672046f58761b82b...|   Aamer Madhani, AP|    WORLD NEWS|2022-09-21|Biden At UN To Ca...|             60|https://www.huffp...|White House offic...|\n|                   1|     58.415354584373375|2022-09-21 00:00:00|               154|            2022|{672046f58761b82b...|   GRAHAM DUNBAR, AP|    WORLD NEWS|2022-09-21|World Cup Captain...|             57|https://www.huffp...|FIFA has come und...|\n|                   4|     58.415354584373375|2022-09-21 00:00:00|               158|            2022|{672046f58761b82b...|  Mari Yamaguchi, AP|    WORLD NEWS|2022-09-21|Man Sets Himself ...|             71|https://www.huffp...|The incident unde...|\n|                   3|     58.415354584373375|2022-09-21 00:00:00|               109|            2022|{672046f58761b82b...|     Dánica Coto, AP|    WORLD NEWS|2022-09-21|Fiona Threatens T...|             60|https://www.huffp...|Hurricane Fiona l...|\n|                  38|     58.415354584373375|2022-09-21 00:00:00|               118|            2022|{672046f58761b82b...|        Ben Blanchet|          TECH|2022-09-21|Twitch Bans Gambl...|             69|https://www.huffp...|One man's claims ...|\n|                   1|     58.415354584373375|2022-09-21 00:00:00|               200|            2022|{672046f58761b82b...|Eric Tucker and M...|     U.S. NEWS|2022-09-21|Virginia Thomas A...|             53|https://www.huffp...|Conservative acti...|\n|                  22|     58.415354584373375|2022-09-20 00:00:00|               115|            2022|{672046f58761b82b...|   Marco Margaritoff|    WORLD NEWS|2022-09-20|Russian Cosmonaut...|             88|https://www.huffp...|Polyakov's record...|\n|                   1|     58.415354584373375|2022-09-20 00:00:00|               130|            2022|{672046f58761b82b...|Marina Fang and C...|CULTURE & ARTS|2022-09-20|'Reboot' Is A Cle...|             67|https://www.huffp...|Starring Keegan-M...|\n|                   2|     58.415354584373375|2022-09-20 00:00:00|               119|            2022|{672046f58761b82b...|     Beth Harris, AP|        SPORTS|2022-09-20|Maury Wills, Base...|             60|https://www.huffp...|Maury Wills, who ...|\n|                   5|     58.415354584373375|2022-09-20 00:00:00|               130|            2022|{672046f58761b82b...|    Jon Gambrell, AP|    WORLD NEWS|2022-09-20|4 Russian-Control...|             78|https://www.huffp...|The concerted and...|\n|                   3|     58.415354584373375|2022-09-20 00:00:00|               154|            2022|{672046f58761b82b...|     Dánica Coto, AP|    WORLD NEWS|2022-09-20|Fiona Barrels Tow...|             69|https://www.huffp...|The Turks and Cai...|\n+--------------------+-----------------------+-------------------+------------------+----------------+--------------------+--------------------+--------------+----------+--------------------+---------------+--------------------+--------------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import length\n",
    "\n",
    "# MongoDB connection details\n",
    "host = \"cluster0.qnu8i.mongodb.net\"  # Removed the comma\n",
    "username = \"adothi1997\"\n",
    "password = \"xm3YvTyKgto0CgVk\"\n",
    "database = \"news_database\"  # Your database name\n",
    "collection = \"news_collection\"  # Your collection name\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"MongoDB Integration\") \\\n",
    "    .config(\"spark.mongodb.input.uri\", f\"mongodb+srv://{username}:{password}@{host}/{database}.{collection}\") \\\n",
    "    .config(\"spark.mongodb.output.uri\", f\"mongodb+srv://{username}:{password}@{host}/{database}.{collection}\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Load data from MongoDB\n",
    "df_spark = spark.read.format(\"com.mongodb.spark.sql.DefaultSource\") \\\n",
    "    .option(\"uri\", f\"mongodb+srv://{username}:{password}@{host}/{database}.{collection}\") \\\n",
    "    .load()\n",
    "\n",
    "# Show the DataFrame\n",
    "df_spark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "416c0c50-d1c3-4f56-97f8-8ee34ed54af3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import length\n",
    "\n",
    "# MongoDB connection details\n",
    "host = \"******\"  # Masked host\n",
    "username = \"******\"  # Masked username\n",
    "password = \"******\"  # Masked password\n",
    "database = \"news_database\"  # Your database name\n",
    "collection = \"news_collection\"  # Your collection name\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"MongoDB Integration\") \\\n",
    "    .config(\"spark.mongodb.input.uri\", f\"mongodb+srv://{username}:{password}@{host}/{database}.{collection}\") \\\n",
    "    .config(\"spark.mongodb.output.uri\", f\"mongodb+srv://{username}:{password}@{host}/{database}.{collection}\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Load data from MongoDB\n",
    "df_spark = spark.read.format(\"com.mongodb.spark.sql.DefaultSource\") \\\n",
    "    .option(\"uri\", f\"mongodb+srv://{username}:{password}@{host}/{database}.{collection}\") \\\n",
    "    .load()\n",
    "\n",
    "# Show the DataFrame\n",
    "df_spark.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9cbdc1bb-6251-4e5b-8cef-71f7c6a4f2d5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Record Count: 209527\n"
     ]
    }
   ],
   "source": [
    "record_count = df_spark.count()\n",
    "print(f\"Record Count: {record_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5ef7ea09-1da7-4e13-88f8-9a07ffafd70e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- Author_Article_Count: long (nullable = true)\n |-- Average_Headline_Length: double (nullable = true)\n |-- Date_Timestamp: timestamp (nullable = true)\n |-- Description_Length: integer (nullable = true)\n |-- Publication_Year: integer (nullable = true)\n |-- _id: struct (nullable = true)\n |    |-- oid: string (nullable = true)\n |-- authors: string (nullable = true)\n |-- category: string (nullable = true)\n |-- date: string (nullable = true)\n |-- headline: string (nullable = true)\n |-- headline_length: integer (nullable = true)\n |-- link: string (nullable = true)\n |-- short_description: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "df_spark.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c28ba906-7141-40e0-ad27-e87ecb80fc7b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------+\n|            headline|   short_description|      category|\n+--------------------+--------------------+--------------+\n|Over 4 Million Am...|Health experts sa...|     U.S. NEWS|\n|American Airlines...|He was subdued by...|     U.S. NEWS|\n|23 Of The Funnies...|\"Until you have a...|        COMEDY|\n|The Funniest Twee...|\"Accidentally put...|     PARENTING|\n|Woman Who Called ...|Amy Cooper accuse...|     U.S. NEWS|\n|Cleaner Was Dead ...|The 63-year-old w...|     U.S. NEWS|\n|Reporter Gets Ado...|\"Who's that behin...|     U.S. NEWS|\n|Puerto Ricans Des...|More than half a ...|    WORLD NEWS|\n|How A New Documen...|In \"Mija,\" direct...|CULTURE & ARTS|\n|Biden At UN To Ca...|White House offic...|    WORLD NEWS|\n|World Cup Captain...|FIFA has come und...|    WORLD NEWS|\n|Man Sets Himself ...|The incident unde...|    WORLD NEWS|\n|Fiona Threatens T...|Hurricane Fiona l...|    WORLD NEWS|\n|Twitch Bans Gambl...|One man's claims ...|          TECH|\n|Virginia Thomas A...|Conservative acti...|     U.S. NEWS|\n|Russian Cosmonaut...|Polyakov's record...|    WORLD NEWS|\n|'Reboot' Is A Cle...|Starring Keegan-M...|CULTURE & ARTS|\n|Maury Wills, Base...|Maury Wills, who ...|        SPORTS|\n|4 Russian-Control...|The concerted and...|    WORLD NEWS|\n|Fiona Barrels Tow...|The Turks and Cai...|    WORLD NEWS|\n+--------------------+--------------------+--------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "# Select specific columns (modify the column names as needed)\n",
    "df_spark_selected = df_spark.select(\"headline\", \"short_description\", \"category\")\n",
    "df_spark_selected.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "88eb3d25-cf37-428c-bb64-50250bb17a15",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------------------+-------------------+------------------+----------------+--------------------+----------------+--------+----------+--------------------+---------------+--------------------+--------------------+\n|Author_Article_Count|Average_Headline_Length|     Date_Timestamp|Description_Length|Publication_Year|                 _id|         authors|category|      date|            headline|headline_length|                link|   short_description|\n+--------------------+-----------------------+-------------------+------------------+----------------+--------------------+----------------+--------+----------+--------------------+---------------+--------------------+--------------------+\n|                 650|     58.415354584373375|2022-09-23 00:00:00|                64|            2022|{672046f58761b82b...|   Elyse Wanshel|  COMEDY|2022-09-23|23 Of The Funnies...|             69|https://www.huffp...|\"Until you have a...|\n|                 650|     58.415354584373375|2022-07-22 00:00:00|                63|            2022|{672046f58761b82b...|   Elyse Wanshel|  COMEDY|2022-07-22|23 Of The Funnies...|             68|https://www.huffp...|“you ever bring u...|\n|                 650|     58.415354584373375|2022-06-25 00:00:00|                86|            2022|{672046f58761b82b...|   Elyse Wanshel|  COMEDY|2022-06-25|20 Of The Funnies...|             68|https://www.huffp...|\"Petition to stop...|\n|                 218|     58.415354584373375|2022-06-16 00:00:00|               111|            2022|{672046f58761b82b...|Josephine Harvey|  COMEDY|2022-06-16|Seth Meyers Has A...|             67|https://www.huffp...|“Sorry, buddy. Yo...|\n|                 650|     58.415354584373375|2022-06-10 00:00:00|               155|            2022|{672046f58761b82b...|   Elyse Wanshel|  COMEDY|2022-06-10|25 Of The Funnies...|             67|https://www.huffp...|\"i keep hearing t...|\n|                1566|     58.415354584373375|2022-04-26 00:00:00|                90|            2022|{672046f58761b82b...|  Mary Papenfuss|  COMEDY|2022-04-26|Jimmy Fallon: Elo...|             63|https://www.huffp...|And former Presid...|\n|                1566|     58.415354584373375|2022-04-10 00:00:00|               121|            2022|{672046f58761b82b...|  Mary Papenfuss|  COMEDY|2022-04-10|'RBG' Hits 'SNL' ...|             84|https://www.huffp...|Kate McKinnon's R...|\n|                1566|     58.415354584373375|2022-04-10 00:00:00|               110|            2022|{672046f58761b82b...|  Mary Papenfuss|  COMEDY|2022-04-10|'Weekend Update' ...|             59|https://www.huffp...|Maybe he should b...|\n|                1566|     58.415354584373375|2022-04-03 00:00:00|               141|            2022|{672046f58761b82b...|  Mary Papenfuss|  COMEDY|2022-04-03|'Will Smith' Supe...|             75|https://www.huffp...|The shocking inci...|\n|                1566|     58.415354584373375|2022-04-03 00:00:00|               136|            2022|{672046f58761b82b...|  Mary Papenfuss|  COMEDY|2022-04-03|'Saturday Night L...|             86|https://www.huffp...|\"Do coup, who do?...|\n|                   7|     58.415354584373375|2022-03-27 00:00:00|               138|            2022|{672046f58761b82b...| Andre Ellington|  COMEDY|2022-03-27|'Saturday Night L...|             83|https://www.huffp...|In an Instagram p...|\n|                1566|     58.415354584373375|2022-03-06 00:00:00|               156|            2022|{672046f58761b82b...|  Mary Papenfuss|  COMEDY|2022-03-06|'Dune' Star Oscar...|             86|https://www.huffp...|\"This was ‘The Av...|\n|                1566|     58.415354584373375|2022-01-23 00:00:00|                72|            2022|{672046f58761b82b...|  Mary Papenfuss|  COMEDY|2022-01-23|'SNL' Spoofs Ferr...|             59|https://www.huffp...|Suggestive sketch...|\n|                 684|     58.415354584373375|2022-01-21 00:00:00|                86|            2022|{672046f58761b82b...|   Hilary Hanson|  COMEDY|2022-01-21|21 Of The Funnies...|             68|https://www.huffp...|\"my toxic trait i...|\n|                 650|     58.415354584373375|2022-01-14 00:00:00|               147|            2022|{672046f58761b82b...|   Elyse Wanshel|  COMEDY|2022-01-14|25 Of The Funnies...|             67|https://www.huffp...|\"At my dog park t...|\n|                 650|     58.415354584373375|2021-12-31 00:00:00|                80|            2021|{672046f58761b82b...|   Elyse Wanshel|  COMEDY|2021-12-31|26 Of The Funnies...|             68|https://www.huffp...|\"Animals are so f...|\n|                1566|     58.415354584373375|2021-12-12 00:00:00|               136|            2021|{672046f58761b82b...|  Mary Papenfuss|  COMEDY|2021-12-12|Kate McKinnon Fin...|             63|https://www.huffp...|Kyle Mooney's San...|\n|                1566|     58.415354584373375|2021-12-12 00:00:00|                85|            2021|{672046f58761b82b...|  Mary Papenfuss|  COMEDY|2021-12-12|Billie Eilish Moc...|             85|https://www.huffp...|She also takes a ...|\n|                1566|     58.415354584373375|2021-11-07 00:00:00|               164|            2021|{672046f58761b82b...|  Mary Papenfuss|  COMEDY|2021-11-07|Kieran Culkin Fin...|             70|https://www.huffp...|“My brother’s up ...|\n|                1566|     58.415354584373375|2021-10-24 00:00:00|               178|            2021|{672046f58761b82b...|  Mary Papenfuss|  COMEDY|2021-10-24|'Joe Biden' Meets...|             71|https://www.huffp...|“I’m you from eig...|\n+--------------------+-----------------------+-------------------+------------------+----------------+--------------------+----------------+--------+----------+--------------------+---------------+--------------------+--------------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "# Filter articles belonging to a specific category (modify 'News' as needed)\n",
    "df_spark_comedy = df_spark.filter(df_spark.category == \"COMEDY\")\n",
    "df_spark_comedy.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "83b854e4-c4fa-48d9-8938-664e7a07eae3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------------------+-------------------+------------------+----------------+--------------------+--------------------+--------------+----------+--------------------+---------------+--------------------+--------------------+\n|Author_Article_Count|Average_Headline_Length|     Date_Timestamp|Description_Length|Publication_Year|                 _id|             authors|      category|      date|            headline|headline_length|                link|   short_description|\n+--------------------+-----------------------+-------------------+------------------+----------------+--------------------+--------------------+--------------+----------+--------------------+---------------+--------------------+--------------------+\n|                   1|     58.415354584373375|2022-09-23 00:00:00|               154|            2022|{672046f58761b82b...|Carla K. Johnson, AP|     U.S. NEWS|2022-09-23|Over 4 Million Am...|             76|https://www.huffp...|Health experts sa...|\n|                1566|     58.415354584373375|2022-09-23 00:00:00|               159|            2022|{672046f58761b82b...|      Mary Papenfuss|     U.S. NEWS|2022-09-23|American Airlines...|             89|https://www.huffp...|He was subdued by...|\n|                 650|     58.415354584373375|2022-09-23 00:00:00|                64|            2022|{672046f58761b82b...|       Elyse Wanshel|        COMEDY|2022-09-23|23 Of The Funnies...|             69|https://www.huffp...|\"Until you have a...|\n|                 889|     58.415354584373375|2022-09-23 00:00:00|               159|            2022|{672046f58761b82b...|    Caroline Bologna|     PARENTING|2022-09-23|The Funniest Twee...|             56|https://www.huffp...|\"Accidentally put...|\n|                1052|     58.415354584373375|2022-09-22 00:00:00|               156|            2022|{672046f58761b82b...|      Nina Golgowski|     U.S. NEWS|2022-09-22|Woman Who Called ...|             77|https://www.huffp...|Amy Cooper accuse...|\n|               37418|     58.415354584373375|2022-09-22 00:00:00|               162|            2022|{672046f58761b82b...|                    |     U.S. NEWS|2022-09-22|Cleaner Was Dead ...|             70|https://www.huffp...|The 63-year-old w...|\n|                 650|     58.415354584373375|2022-09-22 00:00:00|               119|            2022|{672046f58761b82b...|       Elyse Wanshel|     U.S. NEWS|2022-09-22|Reporter Gets Ado...|             67|https://www.huffp...|\"Who's that behin...|\n|                   2|     58.415354584373375|2022-09-22 00:00:00|               116|            2022|{672046f58761b82b...|     DÁNICA COTO, AP|    WORLD NEWS|2022-09-22|Puerto Ricans Des...|             65|https://www.huffp...|More than half a ...|\n|                 696|     58.415354584373375|2022-09-22 00:00:00|               148|            2022|{672046f58761b82b...|         Marina Fang|CULTURE & ARTS|2022-09-22|How A New Documen...|             76|https://www.huffp...|In \"Mija,\" direct...|\n|                   5|     58.415354584373375|2022-09-21 00:00:00|               148|            2022|{672046f58761b82b...|   Aamer Madhani, AP|    WORLD NEWS|2022-09-21|Biden At UN To Ca...|             60|https://www.huffp...|White House offic...|\n|                   1|     58.415354584373375|2022-09-21 00:00:00|               154|            2022|{672046f58761b82b...|   GRAHAM DUNBAR, AP|    WORLD NEWS|2022-09-21|World Cup Captain...|             57|https://www.huffp...|FIFA has come und...|\n|                   4|     58.415354584373375|2022-09-21 00:00:00|               158|            2022|{672046f58761b82b...|  Mari Yamaguchi, AP|    WORLD NEWS|2022-09-21|Man Sets Himself ...|             71|https://www.huffp...|The incident unde...|\n|                   3|     58.415354584373375|2022-09-21 00:00:00|               109|            2022|{672046f58761b82b...|     Dánica Coto, AP|    WORLD NEWS|2022-09-21|Fiona Threatens T...|             60|https://www.huffp...|Hurricane Fiona l...|\n|                  38|     58.415354584373375|2022-09-21 00:00:00|               118|            2022|{672046f58761b82b...|        Ben Blanchet|          TECH|2022-09-21|Twitch Bans Gambl...|             69|https://www.huffp...|One man's claims ...|\n|                   1|     58.415354584373375|2022-09-21 00:00:00|               200|            2022|{672046f58761b82b...|Eric Tucker and M...|     U.S. NEWS|2022-09-21|Virginia Thomas A...|             53|https://www.huffp...|Conservative acti...|\n|                  22|     58.415354584373375|2022-09-20 00:00:00|               115|            2022|{672046f58761b82b...|   Marco Margaritoff|    WORLD NEWS|2022-09-20|Russian Cosmonaut...|             88|https://www.huffp...|Polyakov's record...|\n|                   1|     58.415354584373375|2022-09-20 00:00:00|               130|            2022|{672046f58761b82b...|Marina Fang and C...|CULTURE & ARTS|2022-09-20|'Reboot' Is A Cle...|             67|https://www.huffp...|Starring Keegan-M...|\n|                   2|     58.415354584373375|2022-09-20 00:00:00|               119|            2022|{672046f58761b82b...|     Beth Harris, AP|        SPORTS|2022-09-20|Maury Wills, Base...|             60|https://www.huffp...|Maury Wills, who ...|\n|                   5|     58.415354584373375|2022-09-20 00:00:00|               130|            2022|{672046f58761b82b...|    Jon Gambrell, AP|    WORLD NEWS|2022-09-20|4 Russian-Control...|             78|https://www.huffp...|The concerted and...|\n|                   3|     58.415354584373375|2022-09-20 00:00:00|               154|            2022|{672046f58761b82b...|     Dánica Coto, AP|    WORLD NEWS|2022-09-20|Fiona Barrels Tow...|             69|https://www.huffp...|The Turks and Cai...|\n+--------------------+-----------------------+-------------------+------------------+----------------+--------------------+--------------------+--------------+----------+--------------------+---------------+--------------------+--------------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import length\n",
    "\n",
    "# Add a new column for the length of the headline\n",
    "df_spark = df_spark.withColumn(\"headline_length\", length(df_spark.headline))\n",
    "df_spark.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "768768d2-02cf-4ed5-bd05-428694e66ab2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----+\n|      category|count|\n+--------------+-----+\n|        SPORTS| 5077|\n|         MEDIA| 2944|\n|  BLACK VOICES| 4583|\n|      POLITICS|35602|\n|  QUEER VOICES| 6347|\n|CULTURE & ARTS| 1074|\n|     PARENTING| 8791|\n| ENTERTAINMENT|17362|\n|   ENVIRONMENT| 1444|\n|     U.S. NEWS| 1377|\n|          TECH| 2104|\n|      BUSINESS| 5992|\n| LATINO VOICES| 1130|\n|        COMEDY| 5400|\n|STYLE & BEAUTY| 9814|\n|         MONEY| 1756|\n|        IMPACT| 3484|\n|      RELIGION| 2577|\n|     EDUCATION| 1014|\n|        TRAVEL| 9900|\n+--------------+-----+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "# Group by category and count the number of articles per category\n",
    "df_spark_category = df_spark.groupBy(\"category\").count()\n",
    "df_spark_category.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8e4298bc-eae4-450b-864a-97eb48231a02",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------+----------+--------------------+--------------------+--------------------+---------------+-------------------+----------------+----------+\n|                 _id|             authors|      category|      date|            headline|                link|   short_description|headline_length|     Date_Timestamp|Publication_Year|Word_Count|\n+--------------------+--------------------+--------------+----------+--------------------+--------------------+--------------------+---------------+-------------------+----------------+----------+\n|{672046f58761b82b...|Carla K. Johnson, AP|     U.S. NEWS|2022-09-23|Over 4 Million Am...|https://www.huffp...|Health experts sa...|             76|2022-09-23 00:00:00|            2022|        29|\n|{672046f58761b82b...|      Mary Papenfuss|     U.S. NEWS|2022-09-23|American Airlines...|https://www.huffp...|He was subdued by...|             89|2022-09-23 00:00:00|            2022|        28|\n|{672046f58761b82b...|       Elyse Wanshel|        COMEDY|2022-09-23|23 Of The Funnies...|https://www.huffp...|\"Until you have a...|             69|2022-09-23 00:00:00|            2022|        12|\n|{672046f58761b82b...|    Caroline Bologna|     PARENTING|2022-09-23|The Funniest Twee...|https://www.huffp...|\"Accidentally put...|             56|2022-09-23 00:00:00|            2022|        25|\n|{672046f58761b82b...|      Nina Golgowski|     U.S. NEWS|2022-09-22|Woman Who Called ...|https://www.huffp...|Amy Cooper accuse...|             77|2022-09-22 00:00:00|            2022|        25|\n|{672046f58761b82b...|                    |     U.S. NEWS|2022-09-22|Cleaner Was Dead ...|https://www.huffp...|The 63-year-old w...|             70|2022-09-22 00:00:00|            2022|        26|\n|{672046f58761b82b...|       Elyse Wanshel|     U.S. NEWS|2022-09-22|Reporter Gets Ado...|https://www.huffp...|\"Who's that behin...|             67|2022-09-22 00:00:00|            2022|        20|\n|{672046f58761b82b...|     DÁNICA COTO, AP|    WORLD NEWS|2022-09-22|Puerto Ricans Des...|https://www.huffp...|More than half a ...|             65|2022-09-22 00:00:00|            2022|        19|\n|{672046f58761b82b...|         Marina Fang|CULTURE & ARTS|2022-09-22|How A New Documen...|https://www.huffp...|In \"Mija,\" direct...|             76|2022-09-22 00:00:00|            2022|        22|\n|{672046f58761b82b...|   Aamer Madhani, AP|    WORLD NEWS|2022-09-21|Biden At UN To Ca...|https://www.huffp...|White House offic...|             60|2022-09-21 00:00:00|            2022|        26|\n|{672046f58761b82b...|   GRAHAM DUNBAR, AP|    WORLD NEWS|2022-09-21|World Cup Captain...|https://www.huffp...|FIFA has come und...|             57|2022-09-21 00:00:00|            2022|        24|\n|{672046f58761b82b...|  Mari Yamaguchi, AP|    WORLD NEWS|2022-09-21|Man Sets Himself ...|https://www.huffp...|The incident unde...|             71|2022-09-21 00:00:00|            2022|        26|\n|{672046f58761b82b...|     Dánica Coto, AP|    WORLD NEWS|2022-09-21|Fiona Threatens T...|https://www.huffp...|Hurricane Fiona l...|             60|2022-09-21 00:00:00|            2022|        18|\n|{672046f58761b82b...|        Ben Blanchet|          TECH|2022-09-21|Twitch Bans Gambl...|https://www.huffp...|One man's claims ...|             69|2022-09-21 00:00:00|            2022|        19|\n|{672046f58761b82b...|Eric Tucker and M...|     U.S. NEWS|2022-09-21|Virginia Thomas A...|https://www.huffp...|Conservative acti...|             53|2022-09-21 00:00:00|            2022|        29|\n|{672046f58761b82b...|   Marco Margaritoff|    WORLD NEWS|2022-09-20|Russian Cosmonaut...|https://www.huffp...|Polyakov's record...|             88|2022-09-20 00:00:00|            2022|        18|\n|{672046f58761b82b...|Marina Fang and C...|CULTURE & ARTS|2022-09-20|'Reboot' Is A Cle...|https://www.huffp...|Starring Keegan-M...|             67|2022-09-20 00:00:00|            2022|        20|\n|{672046f58761b82b...|     Beth Harris, AP|        SPORTS|2022-09-20|Maury Wills, Base...|https://www.huffp...|Maury Wills, who ...|             60|2022-09-20 00:00:00|            2022|        19|\n|{672046f58761b82b...|    Jon Gambrell, AP|    WORLD NEWS|2022-09-20|4 Russian-Control...|https://www.huffp...|The concerted and...|             78|2022-09-20 00:00:00|            2022|        21|\n|{672046f58761b82b...|     Dánica Coto, AP|    WORLD NEWS|2022-09-20|Fiona Barrels Tow...|https://www.huffp...|The Turks and Cai...|             69|2022-09-20 00:00:00|            2022|        24|\n+--------------------+--------------------+--------------+----------+--------------------+--------------------+--------------------+---------------+-------------------+----------------+----------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "df_sorted_by_date = df_with_word_count.orderBy(col(\"Date_Timestamp\").desc())\n",
    "df_sorted_by_date.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "35edbf2c-f052-4616-80c2-2d0b5c13fb23",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import length, avg, col, year\n",
    "\n",
    "# Step 1: Add a column for the length of the \"short_description\"\n",
    "df_spark = df_spark.withColumn(\"Description_Length\", length(col(\"short_description\")))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c2d9b4ba-a586-44be-8b03-c25bd6886b27",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Step 2: Extract the publication year from the \"date\" column and create a new column \"Publication_Year\"\n",
    "df_spark = df_spark.withColumn(\"Date_Timestamp\", to_timestamp(\"date\", \"yyyy-MM-dd\"))\n",
    "df_spark = df_spark.withColumn(\"Publication_Year\", year(col(\"Date_Timestamp\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dbdb07b8-1c76-4022-81f3-08265db4d0b1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Step 3: Calculate the average length of headlines and add it as a constant column (optional)\n",
    "avg_headline_length = df_spark.withColumn(\"Headline_Length\", length(col(\"headline\"))) \\\n",
    "                               .select(avg(\"Headline_Length\")).collect()[0][0]\n",
    "df_spark = df_spark.withColumn(\"Average_Headline_Length\", col(\"Headline_Length\") * 0 + avg_headline_length)  # Constant value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e8499a4e-5695-4769-8c51-36194fde8679",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Step 4: Count the number of articles by each author and add this as a new column\n",
    "author_article_count = df_spark.groupBy(\"authors\").count().withColumnRenamed(\"count\", \"Author_Article_Count\")\n",
    "df_spark = df_spark.join(author_article_count, on=\"authors\", how=\"left\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "29f9e55e-df04-483a-a327-fbed48f5ff3e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-----------------------+-------------------+------------------+----------------+--------------------+--------------+----------+--------------------+---------------+--------------------+--------------------+--------------------+\n|             authors|Author_Article_Count|Average_Headline_Length|     Date_Timestamp|Description_Length|Publication_Year|                 _id|      category|      date|            headline|headline_length|                link|   short_description|Author_Article_Count|\n+--------------------+--------------------+-----------------------+-------------------+------------------+----------------+--------------------+--------------+----------+--------------------+---------------+--------------------+--------------------+--------------------+\n|Eric Tucker and M...|                   1|     58.415354584373375|2022-09-21 00:00:00|               200|            2022|{672046f58761b82b...|     U.S. NEWS|2022-09-21|Virginia Thomas A...|             53|https://www.huffp...|Conservative acti...|                   1|\n|   Marco Margaritoff|                  22|     58.415354584373375|2022-09-20 00:00:00|               115|            2022|{672046f58761b82b...|    WORLD NEWS|2022-09-20|Russian Cosmonaut...|             88|https://www.huffp...|Polyakov's record...|                  22|\n|Marina Fang and C...|                   1|     58.415354584373375|2022-09-20 00:00:00|               130|            2022|{672046f58761b82b...|CULTURE & ARTS|2022-09-20|'Reboot' Is A Cle...|             67|https://www.huffp...|Starring Keegan-M...|                   1|\n|   GRAHAM DUNBAR, AP|                   1|     58.415354584373375|2022-09-21 00:00:00|               154|            2022|{672046f58761b82b...|    WORLD NEWS|2022-09-21|World Cup Captain...|             57|https://www.huffp...|FIFA has come und...|                   1|\n|     DÁNICA COTO, AP|                   2|     58.415354584373375|2022-09-22 00:00:00|               116|            2022|{672046f58761b82b...|    WORLD NEWS|2022-09-22|Puerto Ricans Des...|             65|https://www.huffp...|More than half a ...|                   2|\n|       Elyse Wanshel|                 650|     58.415354584373375|2022-09-23 00:00:00|                64|            2022|{672046f58761b82b...|        COMEDY|2022-09-23|23 Of The Funnies...|             69|https://www.huffp...|\"Until you have a...|                 650|\n|       Elyse Wanshel|                 650|     58.415354584373375|2022-09-22 00:00:00|               119|            2022|{672046f58761b82b...|     U.S. NEWS|2022-09-22|Reporter Gets Ado...|             67|https://www.huffp...|\"Who's that behin...|                 650|\n|      Mary Papenfuss|                1566|     58.415354584373375|2022-09-23 00:00:00|               159|            2022|{672046f58761b82b...|     U.S. NEWS|2022-09-23|American Airlines...|             89|https://www.huffp...|He was subdued by...|                1566|\n|  Mari Yamaguchi, AP|                   4|     58.415354584373375|2022-09-21 00:00:00|               158|            2022|{672046f58761b82b...|    WORLD NEWS|2022-09-21|Man Sets Himself ...|             71|https://www.huffp...|The incident unde...|                   4|\n|         Marina Fang|                 696|     58.415354584373375|2022-09-22 00:00:00|               148|            2022|{672046f58761b82b...|CULTURE & ARTS|2022-09-22|How A New Documen...|             76|https://www.huffp...|In \"Mija,\" direct...|                 696|\n|      Nina Golgowski|                1052|     58.415354584373375|2022-09-22 00:00:00|               156|            2022|{672046f58761b82b...|     U.S. NEWS|2022-09-22|Woman Who Called ...|             77|https://www.huffp...|Amy Cooper accuse...|                1052|\n|     Dánica Coto, AP|                   3|     58.415354584373375|2022-09-21 00:00:00|               109|            2022|{672046f58761b82b...|    WORLD NEWS|2022-09-21|Fiona Threatens T...|             60|https://www.huffp...|Hurricane Fiona l...|                   3|\n|     Dánica Coto, AP|                   3|     58.415354584373375|2022-09-20 00:00:00|               154|            2022|{672046f58761b82b...|    WORLD NEWS|2022-09-20|Fiona Barrels Tow...|             69|https://www.huffp...|The Turks and Cai...|                   3|\n|Carla K. Johnson, AP|                   1|     58.415354584373375|2022-09-23 00:00:00|               154|            2022|{672046f58761b82b...|     U.S. NEWS|2022-09-23|Over 4 Million Am...|             76|https://www.huffp...|Health experts sa...|                   1|\n|     Beth Harris, AP|                   2|     58.415354584373375|2022-09-20 00:00:00|               119|            2022|{672046f58761b82b...|        SPORTS|2022-09-20|Maury Wills, Base...|             60|https://www.huffp...|Maury Wills, who ...|                   2|\n|    Jon Gambrell, AP|                   5|     58.415354584373375|2022-09-20 00:00:00|               130|            2022|{672046f58761b82b...|    WORLD NEWS|2022-09-20|4 Russian-Control...|             78|https://www.huffp...|The concerted and...|                   5|\n|    Caroline Bologna|                 889|     58.415354584373375|2022-09-23 00:00:00|               159|            2022|{672046f58761b82b...|     PARENTING|2022-09-23|The Funniest Twee...|             56|https://www.huffp...|\"Accidentally put...|                 889|\n|        Ben Blanchet|                  38|     58.415354584373375|2022-09-21 00:00:00|               118|            2022|{672046f58761b82b...|          TECH|2022-09-21|Twitch Bans Gambl...|             69|https://www.huffp...|One man's claims ...|                  38|\n|                    |               37418|     58.415354584373375|2022-09-22 00:00:00|               162|            2022|{672046f58761b82b...|     U.S. NEWS|2022-09-22|Cleaner Was Dead ...|             70|https://www.huffp...|The 63-year-old w...|               37418|\n|                    |               37418|     58.415354584373375|2022-09-20 00:00:00|               166|            2022|{672046f58761b82b...| ENTERTAINMENT|2022-09-20|Golden Globes Ret...|             60|https://www.huffp...|For the past 18 m...|               37418|\n+--------------------+--------------------+-----------------------+-------------------+------------------+----------------+--------------------+--------------+----------+--------------------+---------------+--------------------+--------------------+--------------------+\nonly showing top 20 rows\n\nAverage Headline Length: 58.415354584373375\n"
     ]
    }
   ],
   "source": [
    "# Show the final transformed DataFrame\n",
    "df_spark.show()\n",
    "print(f\"Average Headline Length: {avg_headline_length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d3245cdd-49c0-4fef-844a-e7afde8e26ec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mPy4JJavaError\u001B[0m                             Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-1230778021092088>:1\u001B[0m\n",
       "\u001B[0;32m----> 1\u001B[0m \u001B[43mdf_spark\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwrite\u001B[49m\u001B[43m \u001B[49m\u001B[43m\\\u001B[49m\n",
       "\u001B[1;32m      2\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mformat\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mmongo\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[43m\\\u001B[49m\n",
       "\u001B[1;32m      3\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmode\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mappend\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[43m\\\u001B[49m\n",
       "\u001B[1;32m      4\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moption\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43muri\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43murl\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[43m\\\u001B[49m\n",
       "\u001B[1;32m      5\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moption\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mdatabase\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdatabase\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[43m\\\u001B[49m\n",
       "\u001B[1;32m      6\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moption\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mcollection\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcollection\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[43m\\\u001B[49m\n",
       "\u001B[1;32m      7\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msave\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n",
       "\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n",
       "\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n",
       "\u001B[1;32m     51\u001B[0m     )\n",
       "\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/readwriter.py:1395\u001B[0m, in \u001B[0;36mDataFrameWriter.save\u001B[0;34m(self, path, format, mode, partitionBy, **options)\u001B[0m\n",
       "\u001B[1;32m   1393\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mformat(\u001B[38;5;28mformat\u001B[39m)\n",
       "\u001B[1;32m   1394\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m path \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
       "\u001B[0;32m-> 1395\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jwrite\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msave\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m   1396\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[1;32m   1397\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jwrite\u001B[38;5;241m.\u001B[39msave(path)\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n",
       "\u001B[1;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n",
       "\u001B[1;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n",
       "\u001B[0;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n",
       "\u001B[1;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n",
       "\u001B[1;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions.py:228\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n",
       "\u001B[1;32m    226\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdeco\u001B[39m(\u001B[38;5;241m*\u001B[39ma: Any, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkw: Any) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Any:\n",
       "\u001B[1;32m    227\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[0;32m--> 228\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mf\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43ma\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkw\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m    229\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m Py4JJavaError \u001B[38;5;28;01mas\u001B[39;00m e:\n",
       "\u001B[1;32m    230\u001B[0m         converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/protocol.py:326\u001B[0m, in \u001B[0;36mget_return_value\u001B[0;34m(answer, gateway_client, target_id, name)\u001B[0m\n",
       "\u001B[1;32m    324\u001B[0m value \u001B[38;5;241m=\u001B[39m OUTPUT_CONVERTER[\u001B[38;5;28mtype\u001B[39m](answer[\u001B[38;5;241m2\u001B[39m:], gateway_client)\n",
       "\u001B[1;32m    325\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m answer[\u001B[38;5;241m1\u001B[39m] \u001B[38;5;241m==\u001B[39m REFERENCE_TYPE:\n",
       "\u001B[0;32m--> 326\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m Py4JJavaError(\n",
       "\u001B[1;32m    327\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAn error occurred while calling \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39m\n",
       "\u001B[1;32m    328\u001B[0m         \u001B[38;5;28mformat\u001B[39m(target_id, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m, name), value)\n",
       "\u001B[1;32m    329\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[1;32m    330\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m Py4JError(\n",
       "\u001B[1;32m    331\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAn error occurred while calling \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[38;5;124m. Trace:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{3}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39m\n",
       "\u001B[1;32m    332\u001B[0m         \u001B[38;5;28mformat\u001B[39m(target_id, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m, name, value))\n",
       "\n",
       "\u001B[0;31mPy4JJavaError\u001B[0m: An error occurred while calling o3891.save.\n",
       ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 148.0 failed 1 times, most recent failure: Lost task 1.0 in stage 148.0 (TID 177) (ip-10-172-247-100.us-west-2.compute.internal executor driver): com.mongodb.MongoCommandException: Command failed with error 8000 (AtlasError): 'you are over your space quota, using 512 MB of 512 MB' on server cluster0-shard-00-02.qnu8i.mongodb.net:27017. The full response is {\"ok\": 0, \"errmsg\": \"you are over your space quota, using 512 MB of 512 MB\", \"code\": 8000, \"codeName\": \"AtlasError\"}\n",
       "\tat com.mongodb.internal.connection.ProtocolHelper.getCommandFailureException(ProtocolHelper.java:175)\n",
       "\tat com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:302)\n",
       "\tat com.mongodb.internal.connection.InternalStreamConnection.sendAndReceive(InternalStreamConnection.java:258)\n",
       "\tat com.mongodb.internal.connection.UsageTrackingInternalConnection.sendAndReceive(UsageTrackingInternalConnection.java:99)\n",
       "\tat com.mongodb.internal.connection.DefaultConnectionPool$PooledConnection.sendAndReceive(DefaultConnectionPool.java:500)\n",
       "\tat com.mongodb.internal.connection.CommandProtocolImpl.execute(CommandProtocolImpl.java:71)\n",
       "\tat com.mongodb.internal.connection.DefaultServer$DefaultServerProtocolExecutor.execute(DefaultServer.java:224)\n",
       "\tat com.mongodb.internal.connection.DefaultServerConnection.executeProtocol(DefaultServerConnection.java:202)\n",
       "\tat com.mongodb.internal.connection.DefaultServerConnection.command(DefaultServerConnection.java:118)\n",
       "\tat com.mongodb.internal.operation.MixedBulkWriteOperation.executeCommand(MixedBulkWriteOperation.java:419)\n",
       "\tat com.mongodb.internal.operation.MixedBulkWriteOperation.executeBulkWriteBatch(MixedBulkWriteOperation.java:245)\n",
       "\tat com.mongodb.internal.operation.MixedBulkWriteOperation.access$700(MixedBulkWriteOperation.java:71)\n",
       "\tat com.mongodb.internal.operation.MixedBulkWriteOperation$1.call(MixedBulkWriteOperation.java:189)\n",
       "\tat com.mongodb.internal.operation.MixedBulkWriteOperation$1.call(MixedBulkWriteOperation.java:180)\n",
       "\tat com.mongodb.internal.operation.OperationHelper.withReleasableConnection(OperationHelper.java:500)\n",
       "\tat com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:180)\n",
       "\tat com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:71)\n",
       "\tat com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:207)\n",
       "\tat com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:441)\n",
       "\tat com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:421)\n",
       "\tat com.mongodb.spark.MongoSpark$.$anonfun$save$4(MongoSpark.scala:138)\n",
       "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
       "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
       "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
       "\tat com.mongodb.spark.MongoSpark$.$anonfun$save$3(MongoSpark.scala:122)\n",
       "\tat com.mongodb.spark.MongoSpark$.$anonfun$save$3$adapted(MongoSpark.scala:121)\n",
       "\tat com.mongodb.spark.MongoConnector.$anonfun$withCollectionDo$1(MongoConnector.scala:186)\n",
       "\tat com.mongodb.spark.MongoConnector.$anonfun$withDatabaseDo$1(MongoConnector.scala:171)\n",
       "\tat com.mongodb.spark.MongoConnector.withMongoClientDo(MongoConnector.scala:154)\n",
       "\tat com.mongodb.spark.MongoConnector.withDatabaseDo(MongoConnector.scala:171)\n",
       "\tat com.mongodb.spark.MongoConnector.withCollectionDo(MongoConnector.scala:184)\n",
       "\tat com.mongodb.spark.MongoSpark$.$anonfun$save$2(MongoSpark.scala:121)\n",
       "\tat com.mongodb.spark.MongoSpark$.$anonfun$save$2$adapted(MongoSpark.scala:120)\n",
       "\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1059)\n",
       "\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1059)\n",
       "\tat org.apache.spark.SparkContext.$anonfun$runJob$2(SparkContext.scala:2798)\n",
       "\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)\n",
       "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
       "\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)\n",
       "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
       "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)\n",
       "\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:179)\n",
       "\tat org.apache.spark.scheduler.Task.$anonfun$run$5(Task.scala:142)\n",
       "\tat com.databricks.unity.EmptyHandle$.runWithAndClose(UCSHandle.scala:126)\n",
       "\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:142)\n",
       "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
       "\tat org.apache.spark.scheduler.Task.run(Task.scala:97)\n",
       "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:904)\n",
       "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1740)\n",
       "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:907)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
       "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:761)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
       "\tat java.lang.Thread.run(Thread.java:750)\n",
       "\n",
       "Driver stacktrace:\n",
       "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:3440)\n",
       "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:3362)\n",
       "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:3351)\n",
       "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
       "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
       "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
       "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:3351)\n",
       "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1460)\n",
       "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1460)\n",
       "\tat scala.Option.foreach(Option.scala:407)\n",
       "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1460)\n",
       "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3651)\n",
       "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3589)\n",
       "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3577)\n",
       "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:51)\n",
       "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$runJob$1(DAGScheduler.scala:1209)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)\n",
       "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1197)\n",
       "\tat org.apache.spark.SparkContext.runJobInternal(SparkContext.scala:2758)\n",
       "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2741)\n",
       "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2779)\n",
       "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2798)\n",
       "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2823)\n",
       "\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$1(RDD.scala:1059)\n",
       "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:165)\n",
       "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:125)\n",
       "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
       "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:445)\n",
       "\tat org.apache.spark.rdd.RDD.foreachPartition(RDD.scala:1057)\n",
       "\tat com.mongodb.spark.MongoSpark$.save(MongoSpark.scala:120)\n",
       "\tat com.mongodb.spark.MongoSpark$.save(MongoSpark.scala:169)\n",
       "\tat com.mongodb.spark.sql.DefaultSource.createRelation(DefaultSource.scala:70)\n",
       "\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:49)\n",
       "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.$anonfun$sideEffectResult$1(commands.scala:82)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)\n",
       "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:80)\n",
       "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:79)\n",
       "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:91)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.$anonfun$applyOrElse$3(QueryExecution.scala:256)\n",
       "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:165)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.$anonfun$applyOrElse$2(QueryExecution.scala:256)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$9(SQLExecution.scala:258)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:448)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$1(SQLExecution.scala:203)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1073)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:131)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:398)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.$anonfun$applyOrElse$1(QueryExecution.scala:255)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$withMVTagsIfNecessary(QueryExecution.scala:238)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.applyOrElse(QueryExecution.scala:251)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.applyOrElse(QueryExecution.scala:244)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:519)\n",
       "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:106)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:519)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:339)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:335)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:495)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:244)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:395)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:244)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:198)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:189)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:305)\n",
       "\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:964)\n",
       "\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:429)\n",
       "\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:396)\n",
       "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:258)\n",
       "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
       "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
       "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
       "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
       "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
       "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n",
       "\tat py4j.Gateway.invoke(Gateway.java:306)\n",
       "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
       "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
       "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:195)\n",
       "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:115)\n",
       "\tat java.lang.Thread.run(Thread.java:750)\n",
       "Caused by: com.mongodb.MongoCommandException: Command failed with error 8000 (AtlasError): 'you are over your space quota, using 512 MB of 512 MB' on server cluster0-shard-00-02.qnu8i.mongodb.net:27017. The full response is {\"ok\": 0, \"errmsg\": \"you are over your space quota, using 512 MB of 512 MB\", \"code\": 8000, \"codeName\": \"AtlasError\"}\n",
       "\tat com.mongodb.internal.connection.ProtocolHelper.getCommandFailureException(ProtocolHelper.java:175)\n",
       "\tat com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:302)\n",
       "\tat com.mongodb.internal.connection.InternalStreamConnection.sendAndReceive(InternalStreamConnection.java:258)\n",
       "\tat com.mongodb.internal.connection.UsageTrackingInternalConnection.sendAndReceive(UsageTrackingInternalConnection.java:99)\n",
       "\tat com.mongodb.internal.connection.DefaultConnectionPool$PooledConnection.sendAndReceive(DefaultConnectionPool.java:500)\n",
       "\tat com.mongodb.internal.connection.CommandProtocolImpl.execute(CommandProtocolImpl.java:71)\n",
       "\tat com.mongodb.internal.connection.DefaultServer$DefaultServerProtocolExecutor.execute(DefaultServer.java:224)\n",
       "\tat com.mongodb.internal.connection.DefaultServerConnection.executeProtocol(DefaultServerConnection.java:202)\n",
       "\tat com.mongodb.internal.connection.DefaultServerConnection.command(DefaultServerConnection.java:118)\n",
       "\tat com.mongodb.internal.operation.MixedBulkWriteOperation.executeCommand(MixedBulkWriteOperation.java:419)\n",
       "\tat com.mongodb.internal.operation.MixedBulkWriteOperation.executeBulkWriteBatch(MixedBulkWriteOperation.java:245)\n",
       "\tat com.mongodb.internal.operation.MixedBulkWriteOperation.access$700(MixedBulkWriteOperation.java:71)\n",
       "\tat com.mongodb.internal.operation.MixedBulkWriteOperation$1.call(MixedBulkWriteOperation.java:189)\n",
       "\tat com.mongodb.internal.operation.MixedBulkWriteOperation$1.call(MixedBulkWriteOperation.java:180)\n",
       "\tat com.mongodb.internal.operation.OperationHelper.withReleasableConnection(OperationHelper.java:500)\n",
       "\tat com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:180)\n",
       "\tat com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:71)\n",
       "\tat com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:207)\n",
       "\tat com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:441)\n",
       "\tat com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:421)\n",
       "\tat com.mongodb.spark.MongoSpark$.$anonfun$save$4(MongoSpark.scala:138)\n",
       "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
       "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
       "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
       "\tat com.mongodb.spark.MongoSpark$.$anonfun$save$3(MongoSpark.scala:122)\n",
       "\tat com.mongodb.spark.MongoSpark$.$anonfun$save$3$adapted(MongoSpark.scala:121)\n",
       "\tat com.mongodb.spark.MongoConnector.$anonfun$withCollectionDo$1(MongoConnector.scala:186)\n",
       "\tat com.mongodb.spark.MongoConnector.$anonfun$withDatabaseDo$1(MongoConnector.scala:171)\n",
       "\tat com.mongodb.spark.MongoConnector.withMongoClientDo(MongoConnector.scala:154)\n",
       "\tat com.mongodb.spark.MongoConnector.withDatabaseDo(MongoConnector.scala:171)\n",
       "\tat com.mongodb.spark.MongoConnector.withCollectionDo(MongoConnector.scala:184)\n",
       "\tat com.mongodb.spark.MongoSpark$.$anonfun$save$2(MongoSpark.scala:121)\n",
       "\tat com.mongodb.spark.MongoSpark$.$anonfun$save$2$adapted(MongoSpark.scala:120)\n",
       "\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1059)\n",
       "\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1059)\n",
       "\tat org.apache.spark.SparkContext.$anonfun$runJob$2(SparkContext.scala:2798)\n",
       "\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)\n",
       "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
       "\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)\n",
       "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
       "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)\n",
       "\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:179)\n",
       "\tat org.apache.spark.scheduler.Task.$anonfun$run$5(Task.scala:142)\n",
       "\tat com.databricks.unity.EmptyHandle$.runWithAndClose(UCSHandle.scala:126)\n",
       "\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:142)\n",
       "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
       "\tat org.apache.spark.scheduler.Task.run(Task.scala:97)\n",
       "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:904)\n",
       "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1740)\n",
       "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:907)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
       "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:761)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
       "\t... 1 more\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mPy4JJavaError\u001B[0m                             Traceback (most recent call last)\nFile \u001B[0;32m<command-1230778021092088>:1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mdf_spark\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwrite\u001B[49m\u001B[43m \u001B[49m\u001B[43m\\\u001B[49m\n\u001B[1;32m      2\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mformat\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mmongo\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[43m\\\u001B[49m\n\u001B[1;32m      3\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmode\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mappend\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[43m\\\u001B[49m\n\u001B[1;32m      4\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moption\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43muri\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43murl\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[43m\\\u001B[49m\n\u001B[1;32m      5\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moption\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mdatabase\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdatabase\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[43m\\\u001B[49m\n\u001B[1;32m      6\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moption\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mcollection\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcollection\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[43m\\\u001B[49m\n\u001B[1;32m      7\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msave\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n\u001B[1;32m     51\u001B[0m     )\n\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/sql/readwriter.py:1395\u001B[0m, in \u001B[0;36mDataFrameWriter.save\u001B[0;34m(self, path, format, mode, partitionBy, **options)\u001B[0m\n\u001B[1;32m   1393\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mformat(\u001B[38;5;28mformat\u001B[39m)\n\u001B[1;32m   1394\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m path \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m-> 1395\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jwrite\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msave\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1396\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   1397\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jwrite\u001B[38;5;241m.\u001B[39msave(path)\n\nFile \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[1;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[0;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[1;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions.py:228\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    226\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdeco\u001B[39m(\u001B[38;5;241m*\u001B[39ma: Any, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkw: Any) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Any:\n\u001B[1;32m    227\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 228\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mf\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43ma\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkw\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    229\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m Py4JJavaError \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    230\u001B[0m         converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n\nFile \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/protocol.py:326\u001B[0m, in \u001B[0;36mget_return_value\u001B[0;34m(answer, gateway_client, target_id, name)\u001B[0m\n\u001B[1;32m    324\u001B[0m value \u001B[38;5;241m=\u001B[39m OUTPUT_CONVERTER[\u001B[38;5;28mtype\u001B[39m](answer[\u001B[38;5;241m2\u001B[39m:], gateway_client)\n\u001B[1;32m    325\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m answer[\u001B[38;5;241m1\u001B[39m] \u001B[38;5;241m==\u001B[39m REFERENCE_TYPE:\n\u001B[0;32m--> 326\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m Py4JJavaError(\n\u001B[1;32m    327\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAn error occurred while calling \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39m\n\u001B[1;32m    328\u001B[0m         \u001B[38;5;28mformat\u001B[39m(target_id, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m, name), value)\n\u001B[1;32m    329\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    330\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m Py4JError(\n\u001B[1;32m    331\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAn error occurred while calling \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[38;5;124m. Trace:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{3}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39m\n\u001B[1;32m    332\u001B[0m         \u001B[38;5;28mformat\u001B[39m(target_id, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m, name, value))\n\n\u001B[0;31mPy4JJavaError\u001B[0m: An error occurred while calling o3891.save.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 148.0 failed 1 times, most recent failure: Lost task 1.0 in stage 148.0 (TID 177) (ip-10-172-247-100.us-west-2.compute.internal executor driver): com.mongodb.MongoCommandException: Command failed with error 8000 (AtlasError): 'you are over your space quota, using 512 MB of 512 MB' on server cluster0-shard-00-02.qnu8i.mongodb.net:27017. The full response is {\"ok\": 0, \"errmsg\": \"you are over your space quota, using 512 MB of 512 MB\", \"code\": 8000, \"codeName\": \"AtlasError\"}\n\tat com.mongodb.internal.connection.ProtocolHelper.getCommandFailureException(ProtocolHelper.java:175)\n\tat com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:302)\n\tat com.mongodb.internal.connection.InternalStreamConnection.sendAndReceive(InternalStreamConnection.java:258)\n\tat com.mongodb.internal.connection.UsageTrackingInternalConnection.sendAndReceive(UsageTrackingInternalConnection.java:99)\n\tat com.mongodb.internal.connection.DefaultConnectionPool$PooledConnection.sendAndReceive(DefaultConnectionPool.java:500)\n\tat com.mongodb.internal.connection.CommandProtocolImpl.execute(CommandProtocolImpl.java:71)\n\tat com.mongodb.internal.connection.DefaultServer$DefaultServerProtocolExecutor.execute(DefaultServer.java:224)\n\tat com.mongodb.internal.connection.DefaultServerConnection.executeProtocol(DefaultServerConnection.java:202)\n\tat com.mongodb.internal.connection.DefaultServerConnection.command(DefaultServerConnection.java:118)\n\tat com.mongodb.internal.operation.MixedBulkWriteOperation.executeCommand(MixedBulkWriteOperation.java:419)\n\tat com.mongodb.internal.operation.MixedBulkWriteOperation.executeBulkWriteBatch(MixedBulkWriteOperation.java:245)\n\tat com.mongodb.internal.operation.MixedBulkWriteOperation.access$700(MixedBulkWriteOperation.java:71)\n\tat com.mongodb.internal.operation.MixedBulkWriteOperation$1.call(MixedBulkWriteOperation.java:189)\n\tat com.mongodb.internal.operation.MixedBulkWriteOperation$1.call(MixedBulkWriteOperation.java:180)\n\tat com.mongodb.internal.operation.OperationHelper.withReleasableConnection(OperationHelper.java:500)\n\tat com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:180)\n\tat com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:71)\n\tat com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:207)\n\tat com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:441)\n\tat com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:421)\n\tat com.mongodb.spark.MongoSpark$.$anonfun$save$4(MongoSpark.scala:138)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat com.mongodb.spark.MongoSpark$.$anonfun$save$3(MongoSpark.scala:122)\n\tat com.mongodb.spark.MongoSpark$.$anonfun$save$3$adapted(MongoSpark.scala:121)\n\tat com.mongodb.spark.MongoConnector.$anonfun$withCollectionDo$1(MongoConnector.scala:186)\n\tat com.mongodb.spark.MongoConnector.$anonfun$withDatabaseDo$1(MongoConnector.scala:171)\n\tat com.mongodb.spark.MongoConnector.withMongoClientDo(MongoConnector.scala:154)\n\tat com.mongodb.spark.MongoConnector.withDatabaseDo(MongoConnector.scala:171)\n\tat com.mongodb.spark.MongoConnector.withCollectionDo(MongoConnector.scala:184)\n\tat com.mongodb.spark.MongoSpark$.$anonfun$save$2(MongoSpark.scala:121)\n\tat com.mongodb.spark.MongoSpark$.$anonfun$save$2$adapted(MongoSpark.scala:120)\n\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1059)\n\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1059)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$2(SparkContext.scala:2798)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:179)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$5(Task.scala:142)\n\tat com.databricks.unity.EmptyHandle$.runWithAndClose(UCSHandle.scala:126)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:142)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:97)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:904)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1740)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:907)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:761)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:3440)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:3362)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:3351)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:3351)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1460)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1460)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1460)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3651)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3589)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3577)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:51)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$runJob$1(DAGScheduler.scala:1209)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1197)\n\tat org.apache.spark.SparkContext.runJobInternal(SparkContext.scala:2758)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2741)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2779)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2798)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2823)\n\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$1(RDD.scala:1059)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:165)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:125)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:445)\n\tat org.apache.spark.rdd.RDD.foreachPartition(RDD.scala:1057)\n\tat com.mongodb.spark.MongoSpark$.save(MongoSpark.scala:120)\n\tat com.mongodb.spark.MongoSpark$.save(MongoSpark.scala:169)\n\tat com.mongodb.spark.sql.DefaultSource.createRelation(DefaultSource.scala:70)\n\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:49)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.$anonfun$sideEffectResult$1(commands.scala:82)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:80)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:79)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:91)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.$anonfun$applyOrElse$3(QueryExecution.scala:256)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:165)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.$anonfun$applyOrElse$2(QueryExecution.scala:256)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$9(SQLExecution.scala:258)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:448)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$1(SQLExecution.scala:203)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1073)\n\tat org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:131)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:398)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.$anonfun$applyOrElse$1(QueryExecution.scala:255)\n\tat org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$withMVTagsIfNecessary(QueryExecution.scala:238)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.applyOrElse(QueryExecution.scala:251)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.applyOrElse(QueryExecution.scala:244)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:519)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:106)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:519)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:339)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:335)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:495)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:244)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:395)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:244)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:198)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:189)\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:305)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:964)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:429)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:396)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:258)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n\tat py4j.Gateway.invoke(Gateway.java:306)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:195)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:115)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: com.mongodb.MongoCommandException: Command failed with error 8000 (AtlasError): 'you are over your space quota, using 512 MB of 512 MB' on server cluster0-shard-00-02.qnu8i.mongodb.net:27017. The full response is {\"ok\": 0, \"errmsg\": \"you are over your space quota, using 512 MB of 512 MB\", \"code\": 8000, \"codeName\": \"AtlasError\"}\n\tat com.mongodb.internal.connection.ProtocolHelper.getCommandFailureException(ProtocolHelper.java:175)\n\tat com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:302)\n\tat com.mongodb.internal.connection.InternalStreamConnection.sendAndReceive(InternalStreamConnection.java:258)\n\tat com.mongodb.internal.connection.UsageTrackingInternalConnection.sendAndReceive(UsageTrackingInternalConnection.java:99)\n\tat com.mongodb.internal.connection.DefaultConnectionPool$PooledConnection.sendAndReceive(DefaultConnectionPool.java:500)\n\tat com.mongodb.internal.connection.CommandProtocolImpl.execute(CommandProtocolImpl.java:71)\n\tat com.mongodb.internal.connection.DefaultServer$DefaultServerProtocolExecutor.execute(DefaultServer.java:224)\n\tat com.mongodb.internal.connection.DefaultServerConnection.executeProtocol(DefaultServerConnection.java:202)\n\tat com.mongodb.internal.connection.DefaultServerConnection.command(DefaultServerConnection.java:118)\n\tat com.mongodb.internal.operation.MixedBulkWriteOperation.executeCommand(MixedBulkWriteOperation.java:419)\n\tat com.mongodb.internal.operation.MixedBulkWriteOperation.executeBulkWriteBatch(MixedBulkWriteOperation.java:245)\n\tat com.mongodb.internal.operation.MixedBulkWriteOperation.access$700(MixedBulkWriteOperation.java:71)\n\tat com.mongodb.internal.operation.MixedBulkWriteOperation$1.call(MixedBulkWriteOperation.java:189)\n\tat com.mongodb.internal.operation.MixedBulkWriteOperation$1.call(MixedBulkWriteOperation.java:180)\n\tat com.mongodb.internal.operation.OperationHelper.withReleasableConnection(OperationHelper.java:500)\n\tat com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:180)\n\tat com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:71)\n\tat com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:207)\n\tat com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:441)\n\tat com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:421)\n\tat com.mongodb.spark.MongoSpark$.$anonfun$save$4(MongoSpark.scala:138)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat com.mongodb.spark.MongoSpark$.$anonfun$save$3(MongoSpark.scala:122)\n\tat com.mongodb.spark.MongoSpark$.$anonfun$save$3$adapted(MongoSpark.scala:121)\n\tat com.mongodb.spark.MongoConnector.$anonfun$withCollectionDo$1(MongoConnector.scala:186)\n\tat com.mongodb.spark.MongoConnector.$anonfun$withDatabaseDo$1(MongoConnector.scala:171)\n\tat com.mongodb.spark.MongoConnector.withMongoClientDo(MongoConnector.scala:154)\n\tat com.mongodb.spark.MongoConnector.withDatabaseDo(MongoConnector.scala:171)\n\tat com.mongodb.spark.MongoConnector.withCollectionDo(MongoConnector.scala:184)\n\tat com.mongodb.spark.MongoSpark$.$anonfun$save$2(MongoSpark.scala:121)\n\tat com.mongodb.spark.MongoSpark$.$anonfun$save$2$adapted(MongoSpark.scala:120)\n\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1059)\n\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1059)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$2(SparkContext.scala:2798)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:179)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$5(Task.scala:142)\n\tat com.databricks.unity.EmptyHandle$.runWithAndClose(UCSHandle.scala:126)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:142)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:97)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:904)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1740)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:907)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:761)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n",
       "errorSummary": "org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 148.0 failed 1 times, most recent failure: Lost task 1.0 in stage 148.0 (TID 177) (ip-10-172-247-100.us-west-2.compute.internal executor driver): com.mongodb.MongoCommandException: Command failed with error 8000 (AtlasError): 'you are over your space quota, using 512 MB of 512 MB' on server cluster0-shard-00-02.qnu8i.mongodb.net:27017. The full response is {\"ok\": 0, \"errmsg\": \"you are over your space quota, using 512 MB of 512 MB\", \"code\": 8000, \"codeName\": \"AtlasError\"}",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_spark.write \\\n",
    "    .format(\"mongo\") \\\n",
    "    .mode(\"append\") \\\n",
    "    .option(\"uri\", url) \\\n",
    "    .option(\"database\", database) \\\n",
    "    .option(\"collection\", collection) \\\n",
    "    .save()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2e24d95b-3c3e-42a2-80be-d19998a7378f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Project_1",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
