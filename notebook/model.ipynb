{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: seaborn in c:\\users\\tejaa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (0.13.2)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: numpy!=1.24.0,>=1.20 in c:\\users\\tejaa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from seaborn) (1.26.4)\n",
      "Requirement already satisfied: pandas>=1.2 in c:\\users\\tejaa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from seaborn) (2.2.2)\n",
      "Requirement already satisfied: matplotlib!=3.6.1,>=3.4 in c:\\users\\tejaa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from seaborn) (3.8.4)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\tejaa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.2.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\tejaa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\tejaa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (4.51.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\tejaa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\tejaa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (24.0)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\tejaa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (10.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\tejaa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\tejaa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\tejaa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas>=1.2->seaborn) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\tejaa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas>=1.2->seaborn) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\tejaa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.4->seaborn) (1.16.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.46.0-py3-none-any.whl.metadata (44 kB)\n",
      "Collecting filelock (from transformers)\n",
      "  Downloading filelock-3.16.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting huggingface-hub<1.0,>=0.23.2 (from transformers)\n",
      "  Downloading huggingface_hub-0.26.2-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\tejaa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\tejaa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (24.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\tejaa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Downloading regex-2024.9.11-cp312-cp312-win_amd64.whl.metadata (41 kB)\n",
      "Requirement already satisfied: requests in c:\\users\\tejaa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (2.31.0)\n",
      "Collecting safetensors>=0.4.1 (from transformers)\n",
      "  Downloading safetensors-0.4.5-cp312-none-win_amd64.whl.metadata (3.9 kB)\n",
      "Collecting tokenizers<0.21,>=0.20 (from transformers)\n",
      "  Downloading tokenizers-0.20.1-cp312-none-win_amd64.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\tejaa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (4.66.5)\n",
      "Collecting fsspec>=2023.5.0 (from huggingface-hub<1.0,>=0.23.2->transformers)\n",
      "  Downloading fsspec-2024.10.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\tejaa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.11.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\tejaa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\tejaa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\tejaa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->transformers) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\tejaa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->transformers) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\tejaa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->transformers) (2024.2.2)\n",
      "Downloading transformers-4.46.0-py3-none-any.whl (10.0 MB)\n",
      "   ---------------------------------------- 0.0/10.0 MB ? eta -:--:--\n",
      "   --------------------- ------------------ 5.5/10.0 MB 30.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 10.0/10.0 MB 26.1 MB/s eta 0:00:00\n",
      "Downloading huggingface_hub-0.26.2-py3-none-any.whl (447 kB)\n",
      "Downloading regex-2024.9.11-cp312-cp312-win_amd64.whl (273 kB)\n",
      "Downloading safetensors-0.4.5-cp312-none-win_amd64.whl (286 kB)\n",
      "Downloading tokenizers-0.20.1-cp312-none-win_amd64.whl (2.4 MB)\n",
      "   ---------------------------------------- 0.0/2.4 MB ? eta -:--:--\n",
      "   ---------------------------------------- 2.4/2.4 MB 27.1 MB/s eta 0:00:00\n",
      "Downloading filelock-3.16.1-py3-none-any.whl (16 kB)\n",
      "Downloading fsspec-2024.10.0-py3-none-any.whl (179 kB)\n",
      "Installing collected packages: safetensors, regex, fsspec, filelock, huggingface-hub, tokenizers, transformers\n",
      "Successfully installed filelock-3.16.1 fsspec-2024.10.0 huggingface-hub-0.26.2 regex-2024.9.11 safetensors-0.4.5 tokenizers-0.20.1 transformers-4.46.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pymongo import MongoClient\n",
    "import json\n",
    "import urllib.parse\n",
    "import re\n",
    "from transformers import RobertaTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load credentials from JSON file\n",
    "with open('credentials_mongodb.json') as f:\n",
    "    login = json.load(f)\n",
    "\n",
    "# Assign credentials to variables\n",
    "username = login['username']\n",
    "password = urllib.parse.quote(login['password'])  # Ensure the password is URL encoded\n",
    "host = login['host']\n",
    "\n",
    "# Construct the MongoDB connection string\n",
    "url = f\"mongodb+srv://{username}:{password}@{host}/?retryWrites=true&w=majority\"\n",
    "\n",
    "# MongoDB setup\n",
    "client = MongoClient(url)  # Use the constructed connection string\n",
    "db = client['news_database']  # Replace with your database name\n",
    "collection = db['news_collection']  # Replace with your collection name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data before cleaning:\n",
      "                        _id  \\\n",
      "0  66f08bfd13e93754c8f7305e   \n",
      "1  66f08bfd13e93754c8f7305f   \n",
      "2  66f08bfd13e93754c8f73060   \n",
      "3  66f08bfd13e93754c8f73061   \n",
      "4  66f08bfd13e93754c8f73062   \n",
      "\n",
      "                                                link  \\\n",
      "0  https://www.huffpost.com/entry/covid-boosters-...   \n",
      "1  https://www.huffpost.com/entry/american-airlin...   \n",
      "2  https://www.huffpost.com/entry/funniest-tweets...   \n",
      "3  https://www.huffpost.com/entry/funniest-parent...   \n",
      "4  https://www.huffpost.com/entry/amy-cooper-lose...   \n",
      "\n",
      "                                            headline   category  \\\n",
      "0  Over 4 Million Americans Roll Up Sleeves For O...  U.S. NEWS   \n",
      "1  American Airlines Flyer Charged, Banned For Li...  U.S. NEWS   \n",
      "2  23 Of The Funniest Tweets About Cats And Dogs ...     COMEDY   \n",
      "3  The Funniest Tweets From Parents This Week (Se...  PARENTING   \n",
      "4  Woman Who Called Cops On Black Bird-Watcher Lo...  U.S. NEWS   \n",
      "\n",
      "                                   short_description               authors  \\\n",
      "0  Health experts said it is too early to predict...  Carla K. Johnson, AP   \n",
      "1  He was subdued by passengers and crew when he ...        Mary Papenfuss   \n",
      "2  \"Until you have a dog you don't understand wha...         Elyse Wanshel   \n",
      "3  \"Accidentally put grown-up toothpaste on my to...      Caroline Bologna   \n",
      "4  Amy Cooper accused investment firm Franklin Te...        Nina Golgowski   \n",
      "\n",
      "         date  \n",
      "0  2022-09-23  \n",
      "1  2022-09-23  \n",
      "2  2022-09-23  \n",
      "3  2022-09-23  \n",
      "4  2022-09-22  \n"
     ]
    }
   ],
   "source": [
    "# Fetch all documents from the MongoDB collection\n",
    "articles = list(collection.find())\n",
    "\n",
    "# Convert to a pandas DataFrame\n",
    "df = pd.DataFrame(articles)\n",
    "\n",
    "# Display the first few rows before cleaning\n",
    "print(\"Data before cleaning:\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['news_database',\n",
       " 'sample_airbnb',\n",
       " 'sample_analytics',\n",
       " 'sample_geospatial',\n",
       " 'sample_guides',\n",
       " 'sample_mflix',\n",
       " 'sample_restaurants',\n",
       " 'sample_supplies',\n",
       " 'sample_training',\n",
       " 'sample_weatherdata',\n",
       " 'school',\n",
       " 'shop',\n",
       " 'admin',\n",
       " 'local']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# list all databases\n",
    "client.list_database_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset Information:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 209527 entries, 0 to 209526\n",
      "Data columns (total 7 columns):\n",
      " #   Column             Non-Null Count   Dtype \n",
      "---  ------             --------------   ----- \n",
      " 0   _id                209527 non-null  object\n",
      " 1   link               209527 non-null  object\n",
      " 2   headline           209527 non-null  object\n",
      " 3   category           209527 non-null  object\n",
      " 4   short_description  209527 non-null  object\n",
      " 5   authors            209527 non-null  object\n",
      " 6   date               209527 non-null  object\n",
      "dtypes: object(7)\n",
      "memory usage: 11.2+ MB\n"
     ]
    }
   ],
   "source": [
    "# 1. Dataset Information\n",
    "print(\"\\nDataset Information:\")\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count       209527\n",
       "unique          42\n",
       "top       POLITICS\n",
       "freq         35602\n",
       "Name: category, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Summary of category column\n",
    "\n",
    "df['category'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "# Check if there is at any duplicate in the DataFrame\n",
    "has_duplicates = df.duplicated().any()\n",
    "\n",
    "# Print True if there are duplicates\n",
    "print(has_duplicates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_id</th>\n",
       "      <th>link</th>\n",
       "      <th>headline</th>\n",
       "      <th>category</th>\n",
       "      <th>short_description</th>\n",
       "      <th>authors</th>\n",
       "      <th>date</th>\n",
       "      <th>headline_length</th>\n",
       "      <th>short_desc_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>66f08bfd13e93754c8f7305e</td>\n",
       "      <td>https://www.huffpost.com/entry/covid-boosters-...</td>\n",
       "      <td>Over 4 Million Americans Roll Up Sleeves For O...</td>\n",
       "      <td>U.S. NEWS</td>\n",
       "      <td>Health experts said it is too early to predict...</td>\n",
       "      <td>Carla K. Johnson, AP</td>\n",
       "      <td>2022-09-23</td>\n",
       "      <td>11</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>66f08bfd13e93754c8f7305f</td>\n",
       "      <td>https://www.huffpost.com/entry/american-airlin...</td>\n",
       "      <td>American Airlines Flyer Charged, Banned For Li...</td>\n",
       "      <td>U.S. NEWS</td>\n",
       "      <td>He was subdued by passengers and crew when he ...</td>\n",
       "      <td>Mary Papenfuss</td>\n",
       "      <td>2022-09-23</td>\n",
       "      <td>13</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>66f08bfd13e93754c8f73060</td>\n",
       "      <td>https://www.huffpost.com/entry/funniest-tweets...</td>\n",
       "      <td>23 Of The Funniest Tweets About Cats And Dogs ...</td>\n",
       "      <td>COMEDY</td>\n",
       "      <td>\"Until you have a dog you don't understand wha...</td>\n",
       "      <td>Elyse Wanshel</td>\n",
       "      <td>2022-09-23</td>\n",
       "      <td>13</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>66f08bfd13e93754c8f73061</td>\n",
       "      <td>https://www.huffpost.com/entry/funniest-parent...</td>\n",
       "      <td>The Funniest Tweets From Parents This Week (Se...</td>\n",
       "      <td>PARENTING</td>\n",
       "      <td>\"Accidentally put grown-up toothpaste on my to...</td>\n",
       "      <td>Caroline Bologna</td>\n",
       "      <td>2022-09-23</td>\n",
       "      <td>9</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>66f08bfd13e93754c8f73062</td>\n",
       "      <td>https://www.huffpost.com/entry/amy-cooper-lose...</td>\n",
       "      <td>Woman Who Called Cops On Black Bird-Watcher Lo...</td>\n",
       "      <td>U.S. NEWS</td>\n",
       "      <td>Amy Cooper accused investment firm Franklin Te...</td>\n",
       "      <td>Nina Golgowski</td>\n",
       "      <td>2022-09-22</td>\n",
       "      <td>11</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209522</th>\n",
       "      <td>66f08bff13e93754c8fa62d0</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/rim-ceo-t...</td>\n",
       "      <td>RIM CEO Thorsten Heins' 'Significant' Plans Fo...</td>\n",
       "      <td>TECH</td>\n",
       "      <td>Verizon Wireless and AT&amp;T are already promotin...</td>\n",
       "      <td>Reuters, Reuters</td>\n",
       "      <td>2012-01-28</td>\n",
       "      <td>8</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209523</th>\n",
       "      <td>66f08bff13e93754c8fa62d1</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/maria-sha...</td>\n",
       "      <td>Maria Sharapova Stunned By Victoria Azarenka I...</td>\n",
       "      <td>SPORTS</td>\n",
       "      <td>Afterward, Azarenka, more effusive with the pr...</td>\n",
       "      <td></td>\n",
       "      <td>2012-01-28</td>\n",
       "      <td>10</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209524</th>\n",
       "      <td>66f08bff13e93754c8fa62d2</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/super-bow...</td>\n",
       "      <td>Giants Over Patriots, Jets Over Colts Among  M...</td>\n",
       "      <td>SPORTS</td>\n",
       "      <td>Leading up to Super Bowl XLVI, the most talked...</td>\n",
       "      <td></td>\n",
       "      <td>2012-01-28</td>\n",
       "      <td>16</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209525</th>\n",
       "      <td>66f08bff13e93754c8fa62d3</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/aldon-smi...</td>\n",
       "      <td>Aldon Smith Arrested: 49ers Linebacker Busted ...</td>\n",
       "      <td>SPORTS</td>\n",
       "      <td>CORRECTION: An earlier version of this story i...</td>\n",
       "      <td></td>\n",
       "      <td>2012-01-28</td>\n",
       "      <td>8</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209526</th>\n",
       "      <td>66f08bff13e93754c8fa62d4</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/dwight-ho...</td>\n",
       "      <td>Dwight Howard Rips Teammates After Magic Loss ...</td>\n",
       "      <td>SPORTS</td>\n",
       "      <td>The five-time all-star center tore into his te...</td>\n",
       "      <td></td>\n",
       "      <td>2012-01-28</td>\n",
       "      <td>9</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>209527 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                             _id  \\\n",
       "0       66f08bfd13e93754c8f7305e   \n",
       "1       66f08bfd13e93754c8f7305f   \n",
       "2       66f08bfd13e93754c8f73060   \n",
       "3       66f08bfd13e93754c8f73061   \n",
       "4       66f08bfd13e93754c8f73062   \n",
       "...                          ...   \n",
       "209522  66f08bff13e93754c8fa62d0   \n",
       "209523  66f08bff13e93754c8fa62d1   \n",
       "209524  66f08bff13e93754c8fa62d2   \n",
       "209525  66f08bff13e93754c8fa62d3   \n",
       "209526  66f08bff13e93754c8fa62d4   \n",
       "\n",
       "                                                     link  \\\n",
       "0       https://www.huffpost.com/entry/covid-boosters-...   \n",
       "1       https://www.huffpost.com/entry/american-airlin...   \n",
       "2       https://www.huffpost.com/entry/funniest-tweets...   \n",
       "3       https://www.huffpost.com/entry/funniest-parent...   \n",
       "4       https://www.huffpost.com/entry/amy-cooper-lose...   \n",
       "...                                                   ...   \n",
       "209522  https://www.huffingtonpost.com/entry/rim-ceo-t...   \n",
       "209523  https://www.huffingtonpost.com/entry/maria-sha...   \n",
       "209524  https://www.huffingtonpost.com/entry/super-bow...   \n",
       "209525  https://www.huffingtonpost.com/entry/aldon-smi...   \n",
       "209526  https://www.huffingtonpost.com/entry/dwight-ho...   \n",
       "\n",
       "                                                 headline   category  \\\n",
       "0       Over 4 Million Americans Roll Up Sleeves For O...  U.S. NEWS   \n",
       "1       American Airlines Flyer Charged, Banned For Li...  U.S. NEWS   \n",
       "2       23 Of The Funniest Tweets About Cats And Dogs ...     COMEDY   \n",
       "3       The Funniest Tweets From Parents This Week (Se...  PARENTING   \n",
       "4       Woman Who Called Cops On Black Bird-Watcher Lo...  U.S. NEWS   \n",
       "...                                                   ...        ...   \n",
       "209522  RIM CEO Thorsten Heins' 'Significant' Plans Fo...       TECH   \n",
       "209523  Maria Sharapova Stunned By Victoria Azarenka I...     SPORTS   \n",
       "209524  Giants Over Patriots, Jets Over Colts Among  M...     SPORTS   \n",
       "209525  Aldon Smith Arrested: 49ers Linebacker Busted ...     SPORTS   \n",
       "209526  Dwight Howard Rips Teammates After Magic Loss ...     SPORTS   \n",
       "\n",
       "                                        short_description  \\\n",
       "0       Health experts said it is too early to predict...   \n",
       "1       He was subdued by passengers and crew when he ...   \n",
       "2       \"Until you have a dog you don't understand wha...   \n",
       "3       \"Accidentally put grown-up toothpaste on my to...   \n",
       "4       Amy Cooper accused investment firm Franklin Te...   \n",
       "...                                                   ...   \n",
       "209522  Verizon Wireless and AT&T are already promotin...   \n",
       "209523  Afterward, Azarenka, more effusive with the pr...   \n",
       "209524  Leading up to Super Bowl XLVI, the most talked...   \n",
       "209525  CORRECTION: An earlier version of this story i...   \n",
       "209526  The five-time all-star center tore into his te...   \n",
       "\n",
       "                     authors        date  headline_length  short_desc_length  \n",
       "0       Carla K. Johnson, AP  2022-09-23               11                 29  \n",
       "1             Mary Papenfuss  2022-09-23               13                 28  \n",
       "2              Elyse Wanshel  2022-09-23               13                 12  \n",
       "3           Caroline Bologna  2022-09-23                9                 25  \n",
       "4             Nina Golgowski  2022-09-22               11                 25  \n",
       "...                      ...         ...              ...                ...  \n",
       "209522      Reuters, Reuters  2012-01-28                8                 18  \n",
       "209523                        2012-01-28               10                 20  \n",
       "209524                        2012-01-28               16                 24  \n",
       "209525                        2012-01-28                8                 20  \n",
       "209526                        2012-01-28                9                 19  \n",
       "\n",
       "[209527 rows x 9 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4. Visualize Article Lengths (Headline and Short Description)\n",
    "df['headline_length'] = df['headline'].apply(lambda x: len(x.split()))\n",
    "df['short_desc_length'] = df['short_description'].apply(lambda x: len(x.split()))\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  headline  category  short_description\n",
      "Empty Rows Count         6         0              19712\n"
     ]
    }
   ],
   "source": [
    "# List of columns to check for empty values\n",
    "columns_to_check = ['headline', 'category', 'short_description']\n",
    "\n",
    "# Create a dictionary to store the number of empty (NaN or empty string) rows for each column\n",
    "empty_counts = {}\n",
    "\n",
    "# Loop through each column and count the number of empty rows\n",
    "for col in columns_to_check:\n",
    "    empty_counts[col] = (df[col].isnull() | (df[col].astype(str).str.strip() == '')).sum()\n",
    "\n",
    "# Convert the result to a DataFrame and display it\n",
    "empty_counts_df = pd.DataFrame(empty_counts, index=[\"Empty Rows Count\"])\n",
    "\n",
    "print(empty_counts_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill in the missing rows for headline and short_description columns\n",
    "\n",
    "df['headline'] = df['headline'].fillna('Unknown [Missing]')\n",
    "df['headline'] = df['headline'].apply(lambda x: 'Unknown [Missing]' if str(x).strip() == '' else x)\n",
    "\n",
    "df['short_description'] = df['short_description'].fillna('No description available [Missing]')\n",
    "df['short_description'] = df['short_description'].apply(lambda x: 'No description available [Missing]' if str(x).strip() == '' else x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['_id', 'link', 'headline', 'category', 'short_description', 'authors',\n",
       "       'date', 'headline_length', 'short_desc_length'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df= df.drop(['link', 'authors', 'date', 'headline_length', 'short_desc_length'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Data Cleaning ---\n",
    "def clean_text(text):\n",
    "    # Lowercase the text\n",
    "    text = text.lower()\n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "    # Remove special characters, numbers, and punctuations\n",
    "    #text = re.sub(r'[^a-z\\s]', '', text)\n",
    "    return text\n",
    "\n",
    "df['clean_headline'] = df['headline'].apply(clean_text)\n",
    "df['clean_short_description'] = df['short_description'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_id</th>\n",
       "      <th>headline</th>\n",
       "      <th>category</th>\n",
       "      <th>short_description</th>\n",
       "      <th>clean_headline</th>\n",
       "      <th>clean_short_description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>128310</th>\n",
       "      <td>66f08bfe13e93754c8f92594</td>\n",
       "      <td>What If We Were All Family Generation Changers?</td>\n",
       "      <td>IMPACT</td>\n",
       "      <td>What if, in doing so, we won't just create new...</td>\n",
       "      <td>what if we were all family generation changers?</td>\n",
       "      <td>what if, in doing so, we won't just create new...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139983</th>\n",
       "      <td>66f08bfe13e93754c8f9532d</td>\n",
       "      <td>Firestorm At AOL Over Employee Benefit Cuts</td>\n",
       "      <td>BUSINESS</td>\n",
       "      <td>It should have been a glorious week for AOL ch...</td>\n",
       "      <td>firestorm at aol over employee benefit cuts</td>\n",
       "      <td>it should have been a glorious week for aol ch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42339</th>\n",
       "      <td>66f08bfd13e93754c8f7d5c1</td>\n",
       "      <td>Dakota Access Protesters Arrested As Deadline ...</td>\n",
       "      <td>POLITICS</td>\n",
       "      <td>A few protesters who refused to leave remained...</td>\n",
       "      <td>dakota access protesters arrested as deadline ...</td>\n",
       "      <td>a few protesters who refused to leave remained...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131494</th>\n",
       "      <td>66f08bfe13e93754c8f93204</td>\n",
       "      <td>One Glimpse Of These Baby Kit Foxes And You'll...</td>\n",
       "      <td>GREEN</td>\n",
       "      <td>No description available [Missing]</td>\n",
       "      <td>one glimpse of these baby kit foxes and you'll...</td>\n",
       "      <td>no description available [missing]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163649</th>\n",
       "      <td>66f08bfe13e93754c8f9af9f</td>\n",
       "      <td>Mens' Sweat Pheromone, Androstadienone, Influe...</td>\n",
       "      <td>SCIENCE</td>\n",
       "      <td>Scientists didn't know if humans played that g...</td>\n",
       "      <td>mens' sweat pheromone, androstadienone, influe...</td>\n",
       "      <td>scientists didn't know if humans played that g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107939</th>\n",
       "      <td>66f08bfe13e93754c8f8d601</td>\n",
       "      <td>The Great Mitt-Stakes: Who 'Wins' Now That Rom...</td>\n",
       "      <td>POLITICS</td>\n",
       "      <td>No description available [Missing]</td>\n",
       "      <td>the great mitt-stakes: who 'wins' now that rom...</td>\n",
       "      <td>no description available [missing]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97306</th>\n",
       "      <td>66f08bfe13e93754c8f8ac78</td>\n",
       "      <td>Democratic Congressman: Why Is Bernie Sanders ...</td>\n",
       "      <td>POLITICS</td>\n",
       "      <td>No description available [Missing]</td>\n",
       "      <td>democratic congressman: why is bernie sanders ...</td>\n",
       "      <td>no description available [missing]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134246</th>\n",
       "      <td>66f08bfe13e93754c8f93cc4</td>\n",
       "      <td>Understanding Habits Helps You Change Them</td>\n",
       "      <td>WELLNESS</td>\n",
       "      <td>Alcohol abuse is often treated like a complete...</td>\n",
       "      <td>understanding habits helps you change them</td>\n",
       "      <td>alcohol abuse is often treated like a complete...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45288</th>\n",
       "      <td>66f08bfd13e93754c8f7e146</td>\n",
       "      <td>John Lewis To Atlanta: We Cannot Afford To Be ...</td>\n",
       "      <td>POLITICS</td>\n",
       "      <td>An estimated 60,000 people gathered in Atlanta...</td>\n",
       "      <td>john lewis to atlanta: we cannot afford to be ...</td>\n",
       "      <td>an estimated 60,000 people gathered in atlanta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136103</th>\n",
       "      <td>66f08bfe13e93754c8f94405</td>\n",
       "      <td>NLGJA's 2014 'Headlines &amp; Headliners' Benefit ...</td>\n",
       "      <td>QUEER VOICES</td>\n",
       "      <td>Some of media's biggest names gathered at New ...</td>\n",
       "      <td>nlgja's 2014 'headlines &amp; headliners' benefit ...</td>\n",
       "      <td>some of media's biggest names gathered at new ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10476 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                             _id  \\\n",
       "128310  66f08bfe13e93754c8f92594   \n",
       "139983  66f08bfe13e93754c8f9532d   \n",
       "42339   66f08bfd13e93754c8f7d5c1   \n",
       "131494  66f08bfe13e93754c8f93204   \n",
       "163649  66f08bfe13e93754c8f9af9f   \n",
       "...                          ...   \n",
       "107939  66f08bfe13e93754c8f8d601   \n",
       "97306   66f08bfe13e93754c8f8ac78   \n",
       "134246  66f08bfe13e93754c8f93cc4   \n",
       "45288   66f08bfd13e93754c8f7e146   \n",
       "136103  66f08bfe13e93754c8f94405   \n",
       "\n",
       "                                                 headline      category  \\\n",
       "128310    What If We Were All Family Generation Changers?        IMPACT   \n",
       "139983        Firestorm At AOL Over Employee Benefit Cuts      BUSINESS   \n",
       "42339   Dakota Access Protesters Arrested As Deadline ...      POLITICS   \n",
       "131494  One Glimpse Of These Baby Kit Foxes And You'll...         GREEN   \n",
       "163649  Mens' Sweat Pheromone, Androstadienone, Influe...       SCIENCE   \n",
       "...                                                   ...           ...   \n",
       "107939  The Great Mitt-Stakes: Who 'Wins' Now That Rom...      POLITICS   \n",
       "97306   Democratic Congressman: Why Is Bernie Sanders ...      POLITICS   \n",
       "134246         Understanding Habits Helps You Change Them      WELLNESS   \n",
       "45288   John Lewis To Atlanta: We Cannot Afford To Be ...      POLITICS   \n",
       "136103  NLGJA's 2014 'Headlines & Headliners' Benefit ...  QUEER VOICES   \n",
       "\n",
       "                                        short_description  \\\n",
       "128310  What if, in doing so, we won't just create new...   \n",
       "139983  It should have been a glorious week for AOL ch...   \n",
       "42339   A few protesters who refused to leave remained...   \n",
       "131494                 No description available [Missing]   \n",
       "163649  Scientists didn't know if humans played that g...   \n",
       "...                                                   ...   \n",
       "107939                 No description available [Missing]   \n",
       "97306                  No description available [Missing]   \n",
       "134246  Alcohol abuse is often treated like a complete...   \n",
       "45288   An estimated 60,000 people gathered in Atlanta...   \n",
       "136103  Some of media's biggest names gathered at New ...   \n",
       "\n",
       "                                           clean_headline  \\\n",
       "128310    what if we were all family generation changers?   \n",
       "139983        firestorm at aol over employee benefit cuts   \n",
       "42339   dakota access protesters arrested as deadline ...   \n",
       "131494  one glimpse of these baby kit foxes and you'll...   \n",
       "163649  mens' sweat pheromone, androstadienone, influe...   \n",
       "...                                                   ...   \n",
       "107939  the great mitt-stakes: who 'wins' now that rom...   \n",
       "97306   democratic congressman: why is bernie sanders ...   \n",
       "134246         understanding habits helps you change them   \n",
       "45288   john lewis to atlanta: we cannot afford to be ...   \n",
       "136103  nlgja's 2014 'headlines & headliners' benefit ...   \n",
       "\n",
       "                                  clean_short_description  \n",
       "128310  what if, in doing so, we won't just create new...  \n",
       "139983  it should have been a glorious week for aol ch...  \n",
       "42339   a few protesters who refused to leave remained...  \n",
       "131494                 no description available [missing]  \n",
       "163649  scientists didn't know if humans played that g...  \n",
       "...                                                   ...  \n",
       "107939                 no description available [missing]  \n",
       "97306                  no description available [missing]  \n",
       "134246  alcohol abuse is often treated like a complete...  \n",
       "45288   an estimated 60,000 people gathered in atlanta...  \n",
       "136103  some of media's biggest names gathered at new ...  \n",
       "\n",
       "[10476 rows x 6 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Take a random sample (e.g., 5% of the total data)\n",
    "subset_df = df.sample(frac=0.05, random_state=42)  # Adjust frac based on your needs (e.g., 0.1 for 10% of data)\n",
    "\n",
    "# Save or inspect the subset\n",
    "subset_df # To verify the size of the subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  clean_headline  category  clean_short_description\n",
      "Empty Rows Count               0         0                        2\n"
     ]
    }
   ],
   "source": [
    "# List of columns to check for empty values\n",
    "columns_to_check = ['clean_headline', 'category', 'clean_short_description']\n",
    "\n",
    "# Create a dictionary to store the number of empty (NaN or empty string) rows for each column\n",
    "empty_counts = {}\n",
    "\n",
    "# Loop through each column and count the number of empty rows\n",
    "for col in columns_to_check:\n",
    "    empty_counts[col] = (df[col].isnull() | (df[col].astype(str).str.strip() == '')).sum()\n",
    "\n",
    "# Convert the result to a DataFrame and display it\n",
    "empty_counts_df = pd.DataFrame(empty_counts, index=[\"Empty Rows Count\"])\n",
    "\n",
    "print(empty_counts_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping unnecessary columns\n",
    "subset_df = subset_df.drop(['_id', 'headline', 'short_description'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>clean_headline</th>\n",
       "      <th>clean_short_description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>128310</th>\n",
       "      <td>IMPACT</td>\n",
       "      <td>what if we were all family generation changers?</td>\n",
       "      <td>what if, in doing so, we won't just create new...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139983</th>\n",
       "      <td>BUSINESS</td>\n",
       "      <td>firestorm at aol over employee benefit cuts</td>\n",
       "      <td>it should have been a glorious week for aol ch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42339</th>\n",
       "      <td>POLITICS</td>\n",
       "      <td>dakota access protesters arrested as deadline ...</td>\n",
       "      <td>a few protesters who refused to leave remained...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131494</th>\n",
       "      <td>GREEN</td>\n",
       "      <td>one glimpse of these baby kit foxes and you'll...</td>\n",
       "      <td>no description available [missing]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163649</th>\n",
       "      <td>SCIENCE</td>\n",
       "      <td>mens' sweat pheromone, androstadienone, influe...</td>\n",
       "      <td>scientists didn't know if humans played that g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107939</th>\n",
       "      <td>POLITICS</td>\n",
       "      <td>the great mitt-stakes: who 'wins' now that rom...</td>\n",
       "      <td>no description available [missing]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97306</th>\n",
       "      <td>POLITICS</td>\n",
       "      <td>democratic congressman: why is bernie sanders ...</td>\n",
       "      <td>no description available [missing]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134246</th>\n",
       "      <td>WELLNESS</td>\n",
       "      <td>understanding habits helps you change them</td>\n",
       "      <td>alcohol abuse is often treated like a complete...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45288</th>\n",
       "      <td>POLITICS</td>\n",
       "      <td>john lewis to atlanta: we cannot afford to be ...</td>\n",
       "      <td>an estimated 60,000 people gathered in atlanta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136103</th>\n",
       "      <td>QUEER VOICES</td>\n",
       "      <td>nlgja's 2014 'headlines &amp; headliners' benefit ...</td>\n",
       "      <td>some of media's biggest names gathered at new ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10476 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            category                                     clean_headline  \\\n",
       "128310        IMPACT    what if we were all family generation changers?   \n",
       "139983      BUSINESS        firestorm at aol over employee benefit cuts   \n",
       "42339       POLITICS  dakota access protesters arrested as deadline ...   \n",
       "131494         GREEN  one glimpse of these baby kit foxes and you'll...   \n",
       "163649       SCIENCE  mens' sweat pheromone, androstadienone, influe...   \n",
       "...              ...                                                ...   \n",
       "107939      POLITICS  the great mitt-stakes: who 'wins' now that rom...   \n",
       "97306       POLITICS  democratic congressman: why is bernie sanders ...   \n",
       "134246      WELLNESS         understanding habits helps you change them   \n",
       "45288       POLITICS  john lewis to atlanta: we cannot afford to be ...   \n",
       "136103  QUEER VOICES  nlgja's 2014 'headlines & headliners' benefit ...   \n",
       "\n",
       "                                  clean_short_description  \n",
       "128310  what if, in doing so, we won't just create new...  \n",
       "139983  it should have been a glorious week for aol ch...  \n",
       "42339   a few protesters who refused to leave remained...  \n",
       "131494                 no description available [missing]  \n",
       "163649  scientists didn't know if humans played that g...  \n",
       "...                                                   ...  \n",
       "107939                 no description available [missing]  \n",
       "97306                  no description available [missing]  \n",
       "134246  alcohol abuse is often treated like a complete...  \n",
       "45288   an estimated 60,000 people gathered in atlanta...  \n",
       "136103  some of media's biggest names gathered at new ...  \n",
       "\n",
       "[10476 rows x 3 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subset_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 8380 rows\n",
      "Testing set size: 2096 rows\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Define features (e.g., 'headline' or 'short_description') and labels ('category')\n",
    "X = subset_df['clean_headline']  # Or use 'clean_headline' if you want the cleaned version\n",
    "y = subset_df['category']\n",
    "\n",
    "# Split the data into training and testing sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Verify the shape of the splits\n",
    "print(f\"Training set size: {X_train.shape[0]} rows\")\n",
    "print(f\"Testing set size: {X_test.shape[0]} rows\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e022324aec44f5aaeb19d1e5176e9c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tejaa\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\file_download.py:139: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\tejaa\\.cache\\huggingface\\hub\\models--roberta-base. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a09a4eee054e4ebc8b9e7a635789a795",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b12fdc5df21f4f5b87b48f52814c3657",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfee7f042c404af0b5690562b3b5dab8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "990cba66b8db430691e572d3a650d1b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ImportError",
     "evalue": "Unable to convert output to PyTorch tensors format, PyTorch is not installed.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 17\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer(\n\u001b[0;32m      9\u001b[0m         text_data\u001b[38;5;241m.\u001b[39mtolist(),                \u001b[38;5;66;03m# Convert the Pandas Series to a list of texts\u001b[39;00m\n\u001b[0;32m     10\u001b[0m         padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_length\u001b[39m\u001b[38;5;124m'\u001b[39m,              \u001b[38;5;66;03m# Pad sequences to the max_length\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     13\u001b[0m         return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m'\u001b[39m                \u001b[38;5;66;03m# Return PyTorch tensors (you can use 'tf' for TensorFlow)\u001b[39;00m\n\u001b[0;32m     14\u001b[0m     )\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# Tokenize the training and testing data\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m X_train_tokens \u001b[38;5;241m=\u001b[39m \u001b[43mtokenize_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m X_test_tokens \u001b[38;5;241m=\u001b[39m tokenize_data(X_test)\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# Now you have tokenized input IDs and attention masks for both training and testing sets\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[26], line 8\u001b[0m, in \u001b[0;36mtokenize_data\u001b[1;34m(text_data, max_length)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtokenize_data\u001b[39m(text_data, max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m128\u001b[39m):\n\u001b[1;32m----> 8\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtokenizer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtext_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtolist\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m                \u001b[49m\u001b[38;5;66;43;03m# Convert the Pandas Series to a list of texts\u001b[39;49;00m\n\u001b[0;32m     10\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmax_length\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m              \u001b[49m\u001b[38;5;66;43;03m# Pad sequences to the max_length\u001b[39;49;00m\n\u001b[0;32m     11\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m                   \u001b[49m\u001b[38;5;66;43;03m# Truncate sequences that exceed max_length\u001b[39;49;00m\n\u001b[0;32m     12\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\u001b[43m             \u001b[49m\u001b[38;5;66;43;03m# Maximum sequence length\u001b[39;49;00m\n\u001b[0;32m     13\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m                \u001b[49m\u001b[38;5;66;43;03m# Return PyTorch tensors (you can use 'tf' for TensorFlow)\u001b[39;49;00m\n\u001b[0;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\tejaa\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:3021\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.__call__\u001b[1;34m(self, text, text_pair, text_target, text_pair_target, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[0;32m   3019\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_in_target_context_manager:\n\u001b[0;32m   3020\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_switch_to_input_mode()\n\u001b[1;32m-> 3021\u001b[0m     encodings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_one\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_pair\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mall_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3022\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m text_target \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   3023\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_switch_to_target_mode()\n",
      "File \u001b[1;32mc:\\Users\\tejaa\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:3109\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._call_one\u001b[1;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens, **kwargs)\u001b[0m\n\u001b[0;32m   3104\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   3105\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch length of `text`: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(text)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not match batch length of `text_pair`:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   3106\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(text_pair)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   3107\u001b[0m         )\n\u001b[0;32m   3108\u001b[0m     batch_text_or_text_pairs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(text, text_pair)) \u001b[38;5;28;01mif\u001b[39;00m text_pair \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m text\n\u001b[1;32m-> 3109\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_encode_plus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3110\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_text_or_text_pairs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_text_or_text_pairs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3111\u001b[0m \u001b[43m        \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3112\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3113\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtruncation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3114\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3115\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3116\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3117\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3118\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpadding_side\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_side\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3119\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3120\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3121\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3122\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3123\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3124\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3125\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3126\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3127\u001b[0m \u001b[43m        \u001b[49m\u001b[43msplit_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msplit_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3128\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3129\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3130\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   3131\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencode_plus(\n\u001b[0;32m   3132\u001b[0m         text\u001b[38;5;241m=\u001b[39mtext,\n\u001b[0;32m   3133\u001b[0m         text_pair\u001b[38;5;241m=\u001b[39mtext_pair,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3151\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   3152\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\tejaa\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:3311\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.batch_encode_plus\u001b[1;34m(self, batch_text_or_text_pairs, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens, **kwargs)\u001b[0m\n\u001b[0;32m   3301\u001b[0m \u001b[38;5;66;03m# Backward compatibility for 'truncation_strategy', 'pad_to_max_length'\u001b[39;00m\n\u001b[0;32m   3302\u001b[0m padding_strategy, truncation_strategy, max_length, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_padding_truncation_strategies(\n\u001b[0;32m   3303\u001b[0m     padding\u001b[38;5;241m=\u001b[39mpadding,\n\u001b[0;32m   3304\u001b[0m     truncation\u001b[38;5;241m=\u001b[39mtruncation,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3308\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   3309\u001b[0m )\n\u001b[1;32m-> 3311\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_batch_encode_plus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3312\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_text_or_text_pairs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_text_or_text_pairs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3313\u001b[0m \u001b[43m    \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3314\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpadding_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3315\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtruncation_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtruncation_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3316\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3317\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3318\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3319\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3320\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpadding_side\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_side\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3321\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3322\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3323\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3324\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3325\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3326\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3327\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3328\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3329\u001b[0m \u001b[43m    \u001b[49m\u001b[43msplit_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msplit_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3330\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3331\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\tejaa\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\tokenization_utils.py:892\u001b[0m, in \u001b[0;36mPreTrainedTokenizer._batch_encode_plus\u001b[1;34m(self, batch_text_or_text_pairs, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens, **kwargs)\u001b[0m\n\u001b[0;32m    889\u001b[0m     second_ids \u001b[38;5;241m=\u001b[39m get_input_ids(pair_ids) \u001b[38;5;28;01mif\u001b[39;00m pair_ids \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    890\u001b[0m     input_ids\u001b[38;5;241m.\u001b[39mappend((first_ids, second_ids))\n\u001b[1;32m--> 892\u001b[0m batch_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_batch_prepare_for_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    893\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    894\u001b[0m \u001b[43m    \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    895\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpadding_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    896\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtruncation_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtruncation_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    897\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    898\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    899\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    900\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpadding_side\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_side\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    901\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    902\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    903\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    904\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    905\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    906\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    907\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    908\u001b[0m \u001b[43m    \u001b[49m\u001b[43msplit_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msplit_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    909\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    911\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m BatchEncoding(batch_outputs)\n",
      "File \u001b[1;32mc:\\Users\\tejaa\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\tokenization_utils.py:979\u001b[0m, in \u001b[0;36mPreTrainedTokenizer._batch_prepare_for_model\u001b[1;34m(self, batch_ids_pairs, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_length, verbose, split_special_tokens)\u001b[0m\n\u001b[0;32m    968\u001b[0m         batch_outputs[key]\u001b[38;5;241m.\u001b[39mappend(value)\n\u001b[0;32m    970\u001b[0m batch_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpad(\n\u001b[0;32m    971\u001b[0m     batch_outputs,\n\u001b[0;32m    972\u001b[0m     padding\u001b[38;5;241m=\u001b[39mpadding_strategy\u001b[38;5;241m.\u001b[39mvalue,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    976\u001b[0m     return_attention_mask\u001b[38;5;241m=\u001b[39mreturn_attention_mask,\n\u001b[0;32m    977\u001b[0m )\n\u001b[1;32m--> 979\u001b[0m batch_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mBatchEncoding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensor_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    981\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m batch_outputs\n",
      "File \u001b[1;32mc:\\Users\\tejaa\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:240\u001b[0m, in \u001b[0;36mBatchEncoding.__init__\u001b[1;34m(self, data, encoding, tensor_type, prepend_batch_axis, n_sequences)\u001b[0m\n\u001b[0;32m    236\u001b[0m     n_sequences \u001b[38;5;241m=\u001b[39m encoding[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mn_sequences\n\u001b[0;32m    238\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_sequences \u001b[38;5;241m=\u001b[39m n_sequences\n\u001b[1;32m--> 240\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_to_tensors\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprepend_batch_axis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepend_batch_axis\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\tejaa\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:730\u001b[0m, in \u001b[0;36mBatchEncoding.convert_to_tensors\u001b[1;34m(self, tensor_type, prepend_batch_axis)\u001b[0m\n\u001b[0;32m    728\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m tensor_type \u001b[38;5;241m==\u001b[39m TensorType\u001b[38;5;241m.\u001b[39mPYTORCH:\n\u001b[0;32m    729\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_available():\n\u001b[1;32m--> 730\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to convert output to PyTorch tensors format, PyTorch is not installed.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    731\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m    733\u001b[0m     is_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mis_tensor\n",
      "\u001b[1;31mImportError\u001b[0m: Unable to convert output to PyTorch tensors format, PyTorch is not installed."
     ]
    }
   ],
   "source": [
    "from transformers import RobertaTokenizer\n",
    "\n",
    "# Load the RoBERTa tokenizer\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "\n",
    "# Function to tokenize the data\n",
    "def tokenize_data(text_data, max_length=128):\n",
    "    return tokenizer(\n",
    "        text_data.tolist(),                # Convert the Pandas Series to a list of texts\n",
    "        padding='max_length',              # Pad sequences to the max_length\n",
    "        truncation=True,                   # Truncate sequences that exceed max_length\n",
    "        max_length=max_length,             # Maximum sequence length\n",
    "        return_tensors='pt'                # Return PyTorch tensors (you can use 'tf' for TensorFlow)\n",
    "    )\n",
    "\n",
    "# Tokenize the training and testing data\n",
    "X_train_tokens = tokenize_data(X_train)\n",
    "X_test_tokens = tokenize_data(X_test)\n",
    "\n",
    "# Now you have tokenized input IDs and attention masks for both training and testing sets\n",
    "print(f\"Training data tokens shape: {X_train_tokens['input_ids'].shape}\")\n",
    "print(f\"Testing data tokens shape: {X_test_tokens['input_ids'].shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Train RoBERTa model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/Users/khadizatannee/anaconda3/envs/adsc_3610/lib/python3.12/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 524/524 [3:12:29<00:00, 22.04s/it, Loss=1.79]      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 524/524 [43:30<00:00,  4.98s/it, Loss=1.76]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 524/524 [1:01:45<00:00,  7.07s/it, Loss=1.18]   \n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import RobertaForSequenceClassification, AdamW\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from transformers import get_scheduler\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Assuming you have already tokenized X_train_tokens and X_test_tokens\n",
    "\n",
    "# Encode the labels (categories) into numerical format\n",
    "label_encoder = LabelEncoder()\n",
    "y_train_encoded = label_encoder.fit_transform(y_train)\n",
    "y_test_encoded = label_encoder.transform(y_test)\n",
    "\n",
    "# Convert data to TensorDataset\n",
    "train_dataset = TensorDataset(X_train_tokens['input_ids'], X_train_tokens['attention_mask'], torch.tensor(y_train_encoded))\n",
    "test_dataset = TensorDataset(X_test_tokens['input_ids'], X_test_tokens['attention_mask'], torch.tensor(y_test_encoded))\n",
    "\n",
    "# Define data loaders\n",
    "batch_size = 16  # Choose a batch size suitable for your GPU memory\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "# Load the pre-trained RoBERTa model for sequence classification\n",
    "num_labels = len(label_encoder.classes_)  # Number of news categories\n",
    "model = RobertaForSequenceClassification.from_pretrained('roberta-base', num_labels=num_labels)\n",
    "\n",
    "# Move the model to GPU if available\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model.to(device)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "# Training parameters\n",
    "num_epochs = 3\n",
    "total_steps = len(train_dataloader) * num_epochs\n",
    "\n",
    "# Learning rate scheduler\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "# Training loop\n",
    "model.train()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "    progress_bar = tqdm(train_dataloader, desc=\"Training\")\n",
    "    \n",
    "    for batch in progress_bar:\n",
    "        # Unpack the batch and move to the device\n",
    "        input_ids, attention_mask, labels = [b.to(device) for b in batch]\n",
    "        \n",
    "        # Zero the gradients before each training step\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # Update weights\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "\n",
    "        progress_bar.set_postfix({\"Loss\": loss.item()})\n",
    "\n",
    "# Save the model after training\n",
    "model.save_pretrained(\"./roberta-news-category-classifier\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5-fold Cross-validation with separeate test set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'label_encoder' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m train_val_df, test_df \u001b[38;5;241m=\u001b[39m train_test_split(subset_df, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m, stratify\u001b[38;5;241m=\u001b[39msubset_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategory\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Tokenize the train/val and test sets separately\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m y_train_val_encoded \u001b[38;5;241m=\u001b[39m \u001b[43mlabel_encoder\u001b[49m\u001b[38;5;241m.\u001b[39mfit_transform(train_val_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategory\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m      8\u001b[0m y_test_encoded \u001b[38;5;241m=\u001b[39m label_encoder\u001b[38;5;241m.\u001b[39mtransform(test_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategory\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     10\u001b[0m X_train_val_tokens \u001b[38;5;241m=\u001b[39m tokenize_data(train_val_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclean_headline\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'label_encoder' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the dataset into 80% for training+validation and 20% for testing\n",
    "train_val_df, test_df = train_test_split(subset_df, test_size=0.2, random_state=42, stratify=subset_df['category'])\n",
    "\n",
    "# Tokenize the train/val and test sets separately\n",
    "y_train_val_encoded = label_encoder.fit_transform(train_val_df['category'])\n",
    "y_test_encoded = label_encoder.transform(test_df['category'])\n",
    "\n",
    "X_train_val_tokens = tokenize_data(train_val_df['clean_headline'])\n",
    "X_test_tokens = tokenize_data(test_df['clean_headline'])\n",
    "\n",
    "# Create TensorDatasets for training+validation and test\n",
    "train_val_dataset = TensorDataset(X_train_val_tokens['input_ids'], X_train_val_tokens['attention_mask'], torch.tensor(y_train_val_encoded))\n",
    "test_dataset = TensorDataset(X_test_tokens['input_ids'], X_test_tokens['attention_mask'], torch.tensor(y_test_encoded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import torch\n",
    "from transformers import RobertaForSequenceClassification, AdamW\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "# KFold configuration\n",
    "k_folds = 5\n",
    "kf = KFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
    "\n",
    "# List to store performance metrics for each fold\n",
    "accuracy_list = []\n",
    "f1_list = []\n",
    "\n",
    "# Encode labels for train_val_dataset (ensure it's already done when creating the dataset)\n",
    "y_train_val_encoded = label_encoder.fit_transform(train_val_df['category'])\n",
    "\n",
    "# Tokenize the entire train_val dataset\n",
    "X_train_val_tokens = tokenize_data(train_val_df['clean_headline'])\n",
    "\n",
    "# Convert the train_val dataset to TensorDataset\n",
    "train_val_dataset = TensorDataset(X_train_val_tokens['input_ids'], X_train_val_tokens['attention_mask'], torch.tensor(y_train_val_encoded))\n",
    "\n",
    "# Move to the device (GPU if available)\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "# Start K-Fold Cross Validation on train_val_dataset\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(train_val_dataset)):\n",
    "    print(f\"\\nTraining Fold {fold + 1}/{k_folds}\")\n",
    "    \n",
    "    # Create DataLoader for train and validation datasets\n",
    "    train_subset = torch.utils.data.Subset(train_val_dataset, train_idx)\n",
    "    val_subset = torch.utils.data.Subset(train_val_dataset, val_idx)\n",
    "    \n",
    "    train_dataloader = DataLoader(train_subset, batch_size=batch_size, shuffle=True)\n",
    "    val_dataloader = DataLoader(val_subset, batch_size=batch_size)\n",
    "\n",
    "    # Load the RoBERTa model for classification with the correct number of labels\n",
    "    model = RobertaForSequenceClassification.from_pretrained('roberta-base', num_labels=len(label_encoder.classes_))\n",
    "    model.to(device)\n",
    "\n",
    "    # Define optimizer\n",
    "    optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "    \n",
    "    # Training loop (for each fold)\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "        progress_bar = tqdm(train_dataloader, desc=\"Training\")\n",
    "\n",
    "        for batch in progress_bar:\n",
    "            # Unpack batch and move to device\n",
    "            input_ids, attention_mask, labels = [b.to(device) for b in batch]\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            progress_bar.set_postfix({\"Loss\": loss.item()})\n",
    "    \n",
    "    # Evaluation on the validation set\n",
    "    model.eval()  # Switch to evaluation mode\n",
    "\n",
    "    val_labels = []\n",
    "    val_predictions = []\n",
    "\n",
    "    for batch in val_dataloader:\n",
    "        input_ids, attention_mask, labels = [b.to(device) for b in batch]\n",
    "        with torch.no_grad():  # Disable gradient calculation\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs.logits\n",
    "            predictions = torch.argmax(logits, dim=-1)\n",
    "\n",
    "            val_labels.extend(labels.cpu().numpy())\n",
    "            val_predictions.extend(predictions.cpu().numpy())\n",
    "\n",
    "    # Calculate metrics for this fold\n",
    "    accuracy = accuracy_score(val_labels, val_predictions)\n",
    "    f1 = f1_score(val_labels, val_predictions, average='weighted')  # Use weighted average for class imbalance\n",
    "\n",
    "    print(f\"Fold {fold + 1} Accuracy: {accuracy:.4f}, F1 Score: {f1:.4f}\")\n",
    "    \n",
    "    accuracy_list.append(accuracy)\n",
    "    f1_list.append(f1)\n",
    "\n",
    "# Average performance across all folds\n",
    "avg_accuracy = sum(accuracy_list) / k_folds\n",
    "avg_f1 = sum(f1_list) / k_folds\n",
    "\n",
    "print(f\"\\nCross-Validation Results - Average Accuracy: {avg_accuracy:.4f}, Average F1 Score: {avg_f1:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ **K-Fold Cross-Validation:**\n",
    "The dataset train_val_dataset is split into 5 folds using KFold. In each iteration, one of these parts is used as the validation set, and the remaining 4 parts are used as the training set.\n",
    "+ **Model Training & Evaluation:**\n",
    "For each fold, the model is trained on the training subset and then evaluated on the validation subset.\n",
    "The code calculates the accuracy and weighted F1 score for each fold and stores the results.\n",
    "+ **Aggregating Results:**\n",
    "At the end of the loop, the code calculates the average accuracy and F1 score across all 5 folds to get a sense of the overall performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Save the best model and tokenizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model that performed the best during cross-validation\n",
    "#model.save_pretrained(\"./best_roberta_news_classifier\")  #already saved in the model training step.\n",
    "tokenizer.save_pretrained(\"./best_roberta_news_classifier\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model Evaluation on test set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataLoader for the test set\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "# Evaluate the model on the separate test set\n",
    "model.eval()  # Switch to evaluation mode\n",
    "\n",
    "test_labels = []\n",
    "test_predictions = []\n",
    "\n",
    "for batch in test_dataloader:\n",
    "    input_ids, attention_mask, labels = [b.to(device) for b in batch]\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "        predictions = torch.argmax(logits, dim=-1)\n",
    "\n",
    "        test_labels.extend(labels.cpu().numpy())\n",
    "        test_predictions.extend(predictions.cpu().numpy())\n",
    "\n",
    "# Calculate final metrics for the separate test set\n",
    "test_accuracy = accuracy_score(test_labels, test_predictions)\n",
    "test_f1 = f1_score(test_labels, test_predictions, average='weighted')\n",
    "\n",
    "print(f\"Test Set Accuracy: {test_accuracy:.4f}\")\n",
    "print(f\"Test Set F1 Score: {test_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Cross-validation results for each fold (example values, replace with actual values from your CV results)\n",
    "accuracy_cv = [0.55, 0.56, 0.54, 0.57, 0.55]\n",
    "precision_cv = [0.53, 0.54, 0.52, 0.55, 0.53]\n",
    "recall_cv = [0.52, 0.53, 0.51, 0.54, 0.52]\n",
    "f1_cv = [0.51, 0.52, 0.53, 0.50, 0.51]\n",
    "\n",
    "# Calculate average cross-validation metrics\n",
    "avg_accuracy_cv = np.mean(accuracy_cv)\n",
    "avg_precision_cv = np.mean(precision_cv)\n",
    "avg_recall_cv = np.mean(recall_cv)\n",
    "avg_f1_cv = np.mean(f1_cv)\n",
    "\n",
    "# Test set metrics (replace with actual results from your test set evaluation)\n",
    "test_accuracy = accuracy_score(test_labels, test_predictions)\n",
    "test_precision = precision_score(test_labels, test_predictions, average='weighted')\n",
    "test_recall = recall_score(test_labels, test_predictions, average='weighted')\n",
    "test_f1 = f1_score(test_labels, test_predictions, average='weighted')\n",
    "test_confusion_matrix = confusion_matrix(test_labels, test_predictions)\n",
    "\n",
    "# Create Evaluation Matrix\n",
    "evaluation_matrix = {\n",
    "    \"Metric\": [\"Accuracy\", \"Weighted Precision\", \"Weighted Recall\", \"Weighted F1-Score\"],\n",
    "    \"Cross-Validation (Fold 1)\": [accuracy_cv[0], precision_cv[0], recall_cv[0], f1_cv[0]],\n",
    "    \"Cross-Validation (Fold 2)\": [accuracy_cv[1], precision_cv[1], recall_cv[1], f1_cv[1]],\n",
    "    \"Cross-Validation (Fold 3)\": [accuracy_cv[2], precision_cv[2], recall_cv[2], f1_cv[2]],\n",
    "    \"Cross-Validation (Fold 4)\": [accuracy_cv[3], precision_cv[3], recall_cv[3], f1_cv[3]],\n",
    "    \"Cross-Validation (Fold 5)\": [accuracy_cv[4], precision_cv[4], recall_cv[4], f1_cv[4]],\n",
    "    \"Cross-Validation (Average)\": [avg_accuracy_cv, avg_precision_cv, avg_recall_cv, avg_f1_cv],\n",
    "    \"Test Set\": [test_accuracy, test_precision, test_recall, test_f1]\n",
    "}\n",
    "evaluation_df = pd.DataFrame(evaluation_matrix)\n",
    "print(evaluation_df)\n",
    "\n",
    "print(\"\\nConfusion Matrix on Test Set:\")\n",
    "print(test_confusion_matrix)"
   ]
  },
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
